#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
News Monitor â€“ í˜ì´ì§• ê°•í™” & ì•ˆì •í™” ë²„ì „
- ë„¤ì´ë²„ ë‰´ìŠ¤ API 5í˜ì´ì§€(ìµœëŒ€ 100ê±´) í˜ì´ì§• ìˆ˜ì§‘ (NAVER_PAGES/NAVER_DISPLAYë¡œ ì¡°ì ˆ)
- ì‹œíŠ¸ ê·œì¹™(MUST_ALL/MUST_ANY/BLOCK/ê²€ìƒ‰ì–´) ê¸°ë°˜ 1ì°¨ í•„í„°
- ê¸°ê°„ í•„í„°: í™”~ê¸ˆ ì „ì¼ 09:00~ì˜¤ëŠ˜ 09:00, ì›”ìš”ì¼ì€ ê¸ˆ 09:00~ì›” 09:00
- ì£¼ë§(í† /ì¼) ìë™ ìŠ¤í‚µ
- ì œëª© ê¸°ì¤€ ì¤‘ë³µ ì œê±°
- ì¡°ì§ë³„ ëŒ€í‘œ ìµœëŒ€ 3ê±´(ì˜ˆì™¸: ì¹´ì¹´ì˜¤/ë¸Œë¼ì´ì–¸ì„íŒ©íŠ¸/ê¹€ë²”ìˆ˜ëŠ” ë¬´ì œí•œ)
- LLM ë¼ë²¨ëŸ¬(ì˜µì…˜, OPENAI_API_KEY + LLM_ENABLE=true) / ê·œì¹™ ë¼ë²¨ëŸ¬ í´ë°±
- ìŠ¬ë™ ì „ì†¡
"""

from __future__ import annotations

import os
import re
import html
import time
import json
import logging
import hashlib
import requests
import pandas as pd
from io import StringIO
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
from dateutil import parser as dtparser
from zoneinfo import ZoneInfo
from slack_sdk import WebClient
from slack_sdk.errors import SlackApiError
import tldextract

# trafilaturaëŠ” ì„ íƒ(ì—†ìœ¼ë©´ requests+BeautifulSoup í´ë°±)
try:
    import trafilatura  # pip install trafilatura>=1.9.0
    _HAS_TRAFILATURA = True
except Exception:
    _HAS_TRAFILATURA = False

# LLM (OpenAI)ëŠ” ì„ íƒ
try:
    import openai  # pip install openai>=1.40.0
    _HAS_OPENAI = True
except Exception:
    _HAS_OPENAI = False

KST = ZoneInfo("Asia/Seoul")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê³µí†µ ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def now_kst() -> datetime:
    return datetime.now(tz=KST)

def parse_datetime(dt_str: str | None) -> datetime | None:
    if not dt_str:
        return None
    try:
        dt = dtparser.parse(dt_str)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except Exception:
        return None

def to_kst_str(dt: datetime | None) -> str:
    if dt is None:
        return ""
    return dt.astimezone(KST).strftime("%Y-%m-%d %H:%M")

def strip_html(text: str | None) -> str:
    text = html.unescape(text or "")
    return BeautifulSoup(text, "html.parser").get_text(separator=" ", strip=True)

def domain_from_url(url: str | None) -> str:
    if not url:
        return ""
    try:
        ext = tldextract.extract(url)
        parts = [p for p in [ext.domain, ext.suffix] if p]
        return ".".join(parts) if parts else ""
    except Exception:
        return ""

def norm_title(t: str | None) -> str:
    t = strip_html(t or "").lower()
    t = re.sub(r"[\[\]ã€ã€‘()ï¼ˆï¼‰ã€ˆã€‰<>ã€ã€ã€Œã€]", " ", t)
    t = re.sub(r"[^\wê°€-í£\s]", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t

def _get_int_env(name: str, default: int) -> int:
    try:
        return int(os.environ.get(name, str(default)).strip())
    except Exception:
        return default

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì¡°íšŒ êµ¬ê°„ ê³„ì‚° (09:00 KST ê¸°ì¤€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def compute_window_utc(now: datetime | None = None) -> tuple[datetime, datetime]:
    """
    ë§¤ì¼ 09:00 KST ì‹¤í–‰ ê¸°ì¤€
    - í™”~ê¸ˆ: ì „ë‚  09:00 ~ ì˜¤ëŠ˜ 09:00
    - ì›”: ê¸ˆìš”ì¼ 09:00 ~ ì›”ìš”ì¼ 09:00
    """
    now = now or datetime.now(tz=KST)
    anchor_kst = now.astimezone(KST).replace(hour=9, minute=0, second=0, microsecond=0)
    days = 3 if anchor_kst.weekday() == 0 else 1  # 0=ì›”
    start_kst = anchor_kst - timedelta(days=days)
    end_kst = anchor_kst
    return start_kst.astimezone(timezone.utc), end_kst.astimezone(timezone.utc)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‹œíŠ¸ ì½ê¸° & ê·œì¹™
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _split_list(val) -> list[str]:
    if pd.isna(val) or str(val).strip() == "":
        return []
    return [x.strip().lower() for x in str(val).split(",") if x.strip()]

def _query_tokens_from(q: str) -> list[str]:
    if not q:
        return []
    parts = re.split(r'\bOR\b', q, flags=re.IGNORECASE)
    tokens = []
    for p in parts:
        t = p.strip().strip('"').strip("'").lower()
        if t:
            tokens.append(t)
    return tokens

def fetch_org_list() -> list[dict]:
    """
    CSV ì‹œíŠ¸ êµ¬ì¡°(ì˜ˆì‹œ):
      - ì¡°ì§ëª…(ë˜ëŠ” í‘œì‹œëª…)
      - ê²€ìƒ‰ì–´(ì„ íƒ)
      - ìœ í˜•(ORG|PERSON, ì„ íƒ)
      - MUST_ALL, MUST_ANY, BLOCK (ì‰¼í‘œ ë¶„ë¦¬, ì„ íƒ)
    """
    sheet_url = os.environ.get("SHEET_CSV_URL", "").strip()
    if not sheet_url:
        raise RuntimeError("SHEET_CSV_URL env var is not set.")

    resp = requests.get(sheet_url, timeout=30)
    resp.raise_for_status()
    csv_text = resp.content.decode("utf-8", errors="replace")
    df = pd.read_csv(StringIO(csv_text))

    name_col = None
    for candidate in ["ì¡°ì§ëª…", "í‘œì‹œëª…"]:
        if candidate in df.columns:
            name_col = candidate
            break
    if not name_col:
        raise RuntimeError("CSVì—ëŠ” ë°˜ë“œì‹œ 'ì¡°ì§ëª…' ë˜ëŠ” 'í‘œì‹œëª…' ì—´ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    rows: list[dict] = []
    for _, r in df.iterrows():
        display = str(r[name_col]).strip()
        if not display or display.lower() == "nan":
            continue

        query = (str(r.get("ê²€ìƒ‰ì–´", "")) or "").strip() or display
        kind  = str(r.get("ìœ í˜•", "ORG")).strip().upper() or "ORG"

        must_all = _split_list(r.get("MUST_ALL", ""))
        must_any = _split_list(r.get("MUST_ANY", ""))
        block    = _split_list(r.get("BLOCK", ""))

        rows.append({
            "display": display,
            "query": query,
            "kind": kind,
            "must_all": must_all,
            "must_any": must_any,
            "block": block,
            "query_tokens": _query_tokens_from(query),
        })

    # (í‘œì‹œëª…, ê²€ìƒ‰ì–´) ê¸°ì¤€ ì¤‘ë³µ ì œê±°
    seen = set(); uniq = []
    for it in rows:
        key = (it["display"], it["query"])
        if key not in seen:
            uniq.append(it); seen.add(key)
    return uniq

def _contains_all(text: str, toks: list[str]) -> bool:
    return all(t in text for t in toks) if toks else True

def _contains_any(text: str, toks: list[str]) -> bool:
    return any(t in text for t in toks) if toks else True

def _contains_none(text: str, toks: list[str]) -> bool:
    return all(t not in text for t in toks) if toks else True

def is_relevant_by_rule(row_cfg: dict, title: str, summary: str) -> bool:
    text = f"{title} {summary}".lower()
    if row_cfg.get("query_tokens") and not _contains_any(text, row_cfg["query_tokens"]):
        return False
    if not _contains_all(text, row_cfg.get("must_all", [])):
        return False
    ma = row_cfg.get("must_any", [])
    if ma and not _contains_any(text, ma):
        return False
    if not _contains_none(text, row_cfg.get("block", [])):
        return False
    return True

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë³¸ë¬¸ ì¶”ì¶œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_article_text(url: str, timeout: int = 20) -> str:
    if not url:
        return ""
    if _HAS_TRAFILATURA:
        try:
            downloaded = trafilatura.fetch_url(url, no_ssl=True, timeout=timeout)
            if downloaded:
                text = trafilatura.extract(
                    downloaded,
                    include_comments=False,
                    include_tables=False,
                    include_formatting=False,
                    favor_recall=True,
                    deduplicate=True,
                ) or ""
                return text.strip()
        except Exception:
            pass
    # í´ë°±
    try:
        r = requests.get(url, timeout=timeout, headers={"User-Agent": "Mozilla/5.0"})
        r.raise_for_status()
        return strip_html(r.text)[:8000].strip()
    except Exception:
        return ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê²€ìƒ‰ê¸° â€“ í˜ì´ì§• ê°•í™” (ë„¤ì´ë²„)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def search_naver(query: str, display: int = 20, pages: int = 5) -> list[dict]:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ API í˜ì´ì§• ìˆ˜ì§‘
    - display: í˜ì´ì§€ë‹¹ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ 20)
    - pages  : í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ 5 â†’ ìµœëŒ€ 100ê±´)
    í™˜ê²½ë³€ìˆ˜:
      NAVER_DISPLAY (ê¸°ë³¸ 20)
      NAVER_PAGES   (ê¸°ë³¸ 5)
    """
    cid  = os.environ.get("NAVER_CLIENT_ID", "")
    csec = os.environ.get("NAVER_CLIENT_SECRET", "")
    if not cid or not csec:
        return []

    display = _get_int_env("NAVER_DISPLAY", display)
    pages   = _get_int_env("NAVER_PAGES", pages)

    endpoint = "https://openapi.naver.com/v1/search/news.json"
    headers  = {"X-Naver-Client-Id": cid, "X-Naver-Client-Secret": csec}

    results: list[dict] = []
    for p in range(pages):
        start = 1 + p * display  # 1, 21, 41, ...
        params = {
            "query": f"{query}",
            "display": display,
            "start": start,
            "sort": "date",
        }
        try:
            r = requests.get(endpoint, headers=headers, params=params, timeout=20)
            r.raise_for_status()
            items = r.json().get("items", [])
            if not items:
                break
            for it in items:
                title = strip_html(it.get("title"))
                url   = it.get("originallink") or it.get("link")
                pub   = parse_datetime(it.get("pubDate"))
                if not url or not title:
                    continue
                src = domain_from_url(url) or "naver"
                results.append({
                    "title": title,
                    "url": url,
                    "source": src,
                    "published_at": pub,
                    "origin": "naver",
                    "summary": strip_html(it.get("description") or ""),
                })
        except Exception:
            # í•œ í˜ì´ì§€ ì‹¤íŒ¨ëŠ” ê±´ë„ˆë›°ê³  ë‹¤ìŒ í˜ì´ì§€ ì‹œë„
            pass

        time.sleep(0.2)  # ë ˆì´íŠ¸ë¦¬ë°‹ ì™„í™”

    return results

# NewsAPIëŠ” ê·¸ëŒ€ë¡œ(ì˜µì…˜)
def search_newsapi(query: str, window_from_utc: datetime, window_to_utc: datetime,
                   language: str = "ko") -> list[dict]:
    key = os.environ.get("NEWSAPI_KEY", "")
    if not key:
        return []
    endpoint = "https://newsapi.org/v2/everything"
    params = {
        "q": f"{query}",
        "from": window_from_utc.isoformat().replace("+00:00", "Z"),
        "to": window_to_utc.isoformat().replace("+00:00", "Z"),
        "sortBy": "publishedAt",
        "pageSize": 50,
        "language": language,
        "apiKey": key,
    }
    try:
        r = requests.get(endpoint, params=params, timeout=20)
        r.raise_for_status()
        arts = r.json().get("articles", [])
        results = []
        for a in arts:
            title = strip_html(a.get("title"))
            url   = a.get("url")
            pub   = parse_datetime(a.get("publishedAt"))
            src   = (a.get("source") or {}).get("name") or domain_from_url(url)
            if not url or not title:
                continue
            results.append({
                "title": title,
                "url": url,
                "source": src,
                "published_at": pub,
                "origin": "newsapi",
                "summary": strip_html(a.get("description") or a.get("content") or ""),
            })
        return results
    except Exception:
        return []

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¼ë²¨ëŸ¬ (ê·œì¹™ + ì„ íƒì  LLM)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
NEG_KW = [
    "íš¡ë ¹","ë°°ì„","ì‚¬ê¸°","ê³ ë°œ","ê¸°ì†Œ","êµ¬ì†","ìˆ˜ì‚¬","ì••ìˆ˜ìˆ˜ìƒ‰","ì†Œì†¡","ì§•ê³„","ì œì¬",
    "ë²Œê¸ˆ","ê³¼ì§•ê¸ˆ","ë¦¬ì½œ","ê²°í•¨","ì‚¬ë§","ë¶€ìƒ","í­ë°œ","í™”ì¬","ì¶”ë½","ìœ ì¶œ","í•´í‚¹","ëœì„¬ì›¨ì–´",
    "ë¶€ë„","íŒŒì‚°","ì˜ì—…ì •ì§€","í‡´ì¶œ"
]
WATCH_KW = [
    "ì˜í˜¹","ì¡°ì‚¬","ì ê²€","ì‹¬ì‚¬","ê²€í† ","êµ­ê°","ê°ì‚¬","ì—°ê¸°","ì§€ì—°","ìœ ì˜ˆ","ìš°ë ¤","ê²½ê³ ","ë¦¬ìŠ¤í¬","ë³€ë™ì„±","ë¶ˆí™•ì‹¤ì„±"
]
POS_KW = [
    "íˆ¬ììœ ì¹˜","ìˆ˜ìƒ","ì„ ì •","í˜ì‹ ","ì‹ ê¸°ë¡","ìµœëŒ€","ë‹¬ì„±","ì„±ê³¼","í˜‘ë ¥","íŒŒíŠ¸ë„ˆì‹­","mou","ê³„ì•½","ìˆ˜ì£¼",
    "í›„ì›","ì§€ì›","ê¸°ë¶€","ê¸°íƒ","ì¥í•™ê¸ˆ","ë´‰ì‚¬","í™•ëŒ€","ì¦ê°€","ìƒìŠ¹","í˜¸ì¡°","ì§„ì¶œ","ì˜¤í”ˆ","ì¶œì‹œ","ê³µê°œ"
]

def rule_label(title: str, summary: str) -> str:
    """ê°„ë‹¨ ê·œì¹™ ë¼ë²¨ëŸ¬ (ê³¼ë„í•œ ë¹¨ê°• ì™„í™”)"""
    text = f"{title} {summary}".lower()
    neg  = any(k in text for k in NEG_KW)
    pos  = any(k in text for k in POS_KW)
    watch= any(k in text for k in WATCH_KW)

    # ê¸ì • ì‹ í˜¸ê°€ ì¡´ì¬í•˜ê³  ì§ì ‘ì  ë¶€ì • í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì¤‘ë¦½/ê¸ì •
    if pos and not neg:
        return "ğŸ”µ"
    # ì§ì ‘ ë¶€ì •ì€ ì—†ê³  ì£¼ì˜ í‚¤ì›Œë“œë§Œ ìˆìœ¼ë©´ ë…¸ë‘
    if watch and not neg:
        return "ğŸŸ¡"
    # ì§ì ‘ ë¶€ì •ì´ ìˆìœ¼ë©´ ë¹¨ê°•
    if neg:
        return "ğŸ”´"
    # ê¸°ë³¸ì€ ì´ˆë¡
    return "ğŸŸ¢"

def llm_enabled() -> bool:
    flag = os.environ.get("LLM_ENABLE", "").strip().lower()
    enabled = flag in {"1","true","yes","on"}
    return enabled and bool(os.environ.get("OPENAI_API_KEY", "").strip()) and _HAS_OPENAI

def llm_label(display: str, title: str, summary: str, content: str) -> str | None:
    """ì„ íƒì  LLM ë¼ë²¨ëŸ¬. ì‹¤íŒ¨ ì‹œ None."""
    if not llm_enabled():
        return None
    body = (content or "").strip()
    if len(body) > 3500:
        body = body[:3500]
    try:
        client = openai.OpenAI(api_key=os.environ["OPENAI_API_KEY"].strip())
        model = os.environ.get("LLM_MODEL", "gpt-4o-mini")
        prompt = f"""ë‹¹ì‹ ì€ ìœ„ê¸°ê´€ë¦¬ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ê¸°ì‚¬ê°€ ì¡°ì§ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í•œ ê°€ì§€ë¡œë§Œ ë¶„ë¥˜í•˜ì„¸ìš”.
ì¡°ì§: {display}
ì œëª©: {title}
ìš”ì•½: {summary}
ë³¸ë¬¸(ì¼ë¶€): {body}

ê°€ëŠ¥í•œ ë¼ë²¨: ğŸ”µ(positive), ğŸŸ¢(neutral), ğŸŸ¡(monitor), ğŸ”´(negative)
ì •í™•íˆ í•´ë‹¹ ê¸°í˜¸ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì„¸ìš”."""
        resp = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=6,
        )
        out = (resp.choices[0].message.content or "").strip()
        return out if out in {"ğŸ”µ","ğŸŸ¢","ğŸŸ¡","ğŸ”´"} else None
    except Exception:
        return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Slack
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def post_to_slack(lines: list[str]) -> None:
    token = os.environ.get("SLACK_BOT_TOKEN", "").strip()
    channel = os.environ.get("SLACK_CHANNEL", "").strip()
    if not token or not channel:
        raise RuntimeError("SLACK_BOT_TOKEN or SLACK_CHANNEL missing.")
    client = WebClient(token=token)
    text = "\n".join(lines) if lines else "ì˜¤ëŠ˜ì€ ì‹ ê·œë¡œ ê°ì§€ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤."
    client.chat_postMessage(channel=channel, text=text)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# main
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
EXEMPT_UNCAPPED = {"ì¹´ì¹´ì˜¤", "ë¸Œë¼ì´ì–¸ì„íŒ©íŠ¸", "ê¹€ë²”ìˆ˜"}  # ë¬´ì œí•œ
PER_ORG_CAP = 3  # ê·¸ ì™¸ ìµœëŒ€ 3ê±´

def main() -> None:
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")

    # ì£¼ë§ ìŠ¤í‚µ
    if now_kst().weekday() in (5, 6):  # 5=í† , 6=ì¼
        logging.info("Weekend (Sat/Sun) â€“ skipping run.")
        return

    window_from_utc, window_to_utc = compute_window_utc()
    logging.info("Window UTC: %s ~ %s", window_from_utc, window_to_utc)

    rows = fetch_org_list()
    logging.info("Loaded %d targets.", len(rows))

    all_lines: list[str] = []

    for idx, row in enumerate(rows, start=1):
        display = row["display"]
        query   = row["query"]
        logging.info("(%d/%d) Searching: %s | %s", idx, len(rows), display, query)

        # ë„¤ì´ë²„ 5í˜ì´ì§€ ìˆ˜ì§‘ (í™˜ê²½ë³€ìˆ˜ë¡œ ì¡°ì ˆ ê°€ëŠ¥)
        naver_items = search_naver(
            query,
            display=_get_int_env("NAVER_DISPLAY", 20),
            pages=_get_int_env("NAVER_PAGES", 5),
        )
        time.sleep(0.25)
        newsapi_items = search_newsapi(query, window_from_utc, window_to_utc, language="ko")
        logging.info("  raw: naver=%d, newsapi=%d", len(naver_items), len(newsapi_items))

        items: list[dict] = []
        for it in (naver_items + newsapi_items):
            it["display"] = display
            it["row_cfg"] = row
            items.append(it)

        # 1) ê´€ë ¨ì„± í•„í„°
        before_rel = len(items)
        items = [it for it in items if is_relevant_by_rule(it["row_cfg"], it["title"], it.get("summary",""))]
        logging.info("  after relevance: %d -> %d", before_rel, len(items))

        # 2) ê¸°ê°„ í•„í„°
        before_win = len(items)
        items = [it for it in items if it["published_at"] and window_from_utc <= it["published_at"] < window_to_utc]
        logging.info("  after window: %d -> %d", before_win, len(items))

        # 3) ìµœì‹ ìˆœ + ì œëª© ê¸°ì¤€ dedup
        items.sort(key=lambda x: x["published_at"], reverse=True)
        seen_titles = set(); uniq = []
        for it in items:
            tkey = norm_title(it["title"])
            if tkey and it["url"] and tkey not in seen_titles:
                uniq.append(it); seen_titles.add(tkey)

        # 4) ì¡°ì§ë³„ ìƒí•œ ì ìš© (ì˜ˆì™¸ëŠ” ë¬´ì œí•œ)
        if display not in EXEMPT_UNCAPPED:
            uniq = uniq[:PER_ORG_CAP]

        # 5) ë¼ë²¨ë§ & ë©”ì‹œì§€ ìƒì„±
        for art in uniq:
            content = ""  # ì†ë„ ìœ„í•´ ë³¸ë¬¸ì€ ê¸°ë³¸ ë¯¸ì‚¬ìš© (LLM ì“°ë©´ í™œì„±í™”)
            label = None

            if llm_enabled():
                # í•„ìš” ì‹œ ë³¸ë¬¸ ì¶”ì¶œ
                content = fetch_article_text(art["url"])
                label = llm_label(art["display"], art["title"], art.get("summary",""), content)

            if not label:
                label = rule_label(art["title"], art.get("summary",""))

            src = art["source"]
            when_str = to_kst_str(art["published_at"])
            line = f"[{art['display']}] <{art['url']}|{art['title']}> ({src})({when_str}) [{label}]"
            all_lines.append(line)

    post_to_slack(all_lines)
    logging.info("Posted %d lines to Slack.", len(all_lines))


if __name__ == "__main__":
    try:
        main()
    except Exception:
        logging.exception("Fatal error")
        raise
