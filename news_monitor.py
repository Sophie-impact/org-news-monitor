#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
News Monitor ‚Äì Ïª®ÌÖçÏä§Ìä∏+LLM Í∏∞Î∞ò ÎùºÎ≤®ÎßÅ / Ï£ºÎßê Ïä§ÌÇµ / ÎåÄÌëú 3Í±¥ Ï†úÌïú(ÏùºÎ∂Ä Ï†úÏô∏)
ÏöîÏ≤≠ Î∞òÏòÅ:
- ÏòàÎ∞©/ÏÉÅÏãúÌôúÎèô ÏïàÏ†ÑÏã†Ìò∏
- Í∑úÏπô ÎùºÎ≤®Îü¨Ïùò Í≥ºÎèÑÌïú üî¥ ÏôÑÌôî
- LLM ÌÜµÌï© Îã®Í≥ÑÏóêÏÑú Í∏çÏ†ïÏö∞ÏÑ∏+ÏßÅÏ†ëÎ∂ÄÏ†ïÏóÜÏùå ÏôÑÌôî
- Ï°∞ÏßÅÎ≥Ñ ÎåÄÌëú ÏµúÎåÄ 3Í±¥(Ïπ¥Ïπ¥Ïò§/Î∏åÏûÑ/ÍπÄÎ≤îÏàò Ï†úÏô∏)
- Ïä¨Îûô Î≥¥Ï°∞Ï†ïÎ≥¥ Ï†úÍ±∞
"""

from __future__ import annotations

import os
import re
import html
import time
import json
import hashlib
import logging
import requests
import pandas as pd
from io import StringIO
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
from dateutil import parser as dtparser
from zoneinfo import ZoneInfo
from slack_sdk import WebClient
from slack_sdk.errors import SlackApiError  # noqa: F401
import tldextract
import trafilatura
from collections import defaultdict
from typing import Optional, Tuple, List, Dict, Any

# --- LLM (OpenAI) ---
try:
    import openai  # pip install openai>=1.40.0
    _HAS_OPENAI = True
except Exception:
    _HAS_OPENAI = False

KST = ZoneInfo("Asia/Seoul")

# ÎåÄÌëú 3Í±¥ Ï†úÌïú Ï†úÏô∏ ÎåÄÏÉÅ
PRIORITY_ORGS = {"Ïπ¥Ïπ¥Ïò§", "Î∏åÎùºÏù¥Ïñ∏ÏûÑÌå©Ìä∏", "ÍπÄÎ≤îÏàò"}

# =========================
# ÌÇ§ÏõåÎìú(Ïª®ÌÖçÏä§Ìä∏ Î∂ÑÎ•ò)
# =========================
DIRECT_NEGATIVE = {
    "Î≤ïÏ†Å": ["Ìö°Î†π", "Î∞∞ÏûÑ", "ÏÇ¨Í∏∞", "Í≥†Î∞ú", "Í∏∞ÏÜå", "Íµ¨ÏÜç", "ÏàòÏÇ¨", "ÏïïÏàòÏàòÏÉâ", "ÌäπÍ≤Ä", "ÏßïÏó≠", "Ïã§Ìòï"],
    "ÏÇ¨ÏóÖ": ["Î¶¨ÏΩú", "Í≤∞Ìï®", "ÌååÏÇ∞", "Î∂ÄÎèÑ", "ÏòÅÏóÖÏ†ïÏßÄ", "ÏÇ¨ÏóÖÏ§ëÎã®", "Ìá¥Ï∂ú"],
    "ÏïàÏ†Ñ": ["ÏÇ¨Îßù", "Î∂ÄÏÉÅ", "Ìè≠Î∞ú", "ÌôîÏû¨", "Ï∂îÎùΩ", "Ïú†Ï∂ú", "Ìï¥ÌÇπ", "ÎûúÏÑ¨Ïõ®Ïñ¥", "Í∞úÏù∏Ï†ïÎ≥¥Ïú†Ï∂ú"]
}
CONTEXTUAL_NEGATIVE = {
    "Í≤ΩÏòÅ": ["Ï†ÅÏûê", "ÏÜêÏã§", "Í∞êÏÜå", "ÌïòÎùΩ", "Î∂ÄÏã§"],
    "Í∑úÏ†ú": ["Ï†úÏû¨", "Î≤åÍ∏à", "Í≥ºÏßïÍ∏à", "ÏßïÍ≥Ñ", "Ï≤òÎ∂Ñ"],
    "ÎÖºÎûÄ": ["ÎÖºÎûÄ", "ÎπÑÌåê", "Í∞ëÏßà", "Î∂àÎ≤ï", "ÏúÑÎ≤ï", "Î∂ÄÏ†ï"]
}
MONITORING_KEYWORDS = {
    "Ï°∞ÏÇ¨": ["ÏùòÌòπ", "Ï°∞ÏÇ¨", "Ïã¨ÏÇ¨", "Í≤ÄÌÜ†", "Íµ≠Í∞ê", "Í∞êÏÇ¨", "Î∂àÌôïÏã§ÏÑ±", "Ïó∞Í∏∞", "ÏßÄÏó∞", "Ïú†Ïòà", "Ïû†Ï†ï", "Í≤ÄÌÜ†Ï§ë", "Ïö∞Î†§", "Í≤ΩÍ≥†", "Î¶¨Ïä§ÌÅ¨", "Î≥ÄÎèôÏÑ±", "Í¥ÄÏã¨", "Ï£ºÏãú"]
}
# ÏòàÎ∞©/ÏÉÅÏãú ÌôúÎèô ‚Üí ÏïàÏ†Ñ Ïã†Ìò∏(Í∞ïÌïú Í∏çÏ†ï Í∞ÄÏ§ë)
SAFE_ACTIVITY_KEYWORDS = [
    "ÏòàÎ∞©", "Ï†ïÍ∏∞Ï†êÍ≤Ä", "ÏÉÅÏãúÏ†êÍ≤Ä", "ÏïàÏ†ÑÏ†êÍ≤Ä", "Ïã§ÌÉú Ï†êÍ≤Ä", "ÏïàÏ†Ñ ÍµêÏú°", "ÏïàÏ†ÑÍµêÏú°",
    "ÌõàÎ†®", "Î™®ÏùòÌõàÎ†®", "Î¶¨ÌóàÏÑ§", "Ï∫†ÌéòÏù∏", "ÏûêÏú®Ï†êÍ≤Ä", "Ï†êÍ≤Ä Ïã§Ïãú", "Ï†êÍ≤Ä ÏßÑÌñâ", "ÏïàÏ†Ñ Î≥¥Í∞ï",
    "Î≥¥Í±¥Í¥ÄÎ¶¨", "ÏÇ¨Í≥† ÏòàÎ∞©", "ÌäπÎ≥Ñ Ï†êÍ≤Ä", "ÌòÑÏû• Ï†êÍ≤Ä", "ÏßëÏ§ë Ï†êÍ≤Ä"
]
POSITIVE_KEYWORDS = {
    "ÏÑ±Í≥º": ["ÏàòÏÉÅ", "ÏÑ†Ï†ï", "ÌòÅÏã†", "Ïã†Í∏∞Î°ù", "ÏµúÎåÄ", "Îã¨ÏÑ±", "ÏÑ±Í≥º", "ÌùëÏûêÏ†ÑÌôò"],
    "ÏÑ±Ïû•": ["Ìà¨ÏûêÏú†Ïπò", "ÏãúÎ¶¨Ï¶à", "ÏÉÅÏäπ", "Ï¶ùÍ∞Ä", "Ìò∏Ï°∞", "ÌôïÎåÄ", "ÏßÑÏ∂ú", "ÏÑ±Ïû•"],
    "ÌòëÎ†•": ["ÌòëÎ†•", "ÌååÌä∏ÎÑàÏã≠", "mou", "Í≥ÑÏïΩ", "ÏàòÏ£º", "Ï†úÌú¥", "Ïó∞Ìï©"],
    "ÏÇ¨ÌöåÍ≥µÌóå": ["ÌõÑÏõê", "ÏßÄÏõê", "Í∏∞Î∂Ä", "Í∏∞Ï¶ù", "Í∏∞ÌÉÅ", "Ïû•ÌïôÍ∏à", "Î¥âÏÇ¨"]
}

# =========================
# Í≥µÌÜµ Ïú†Ìã∏
# =========================
def now_kst() -> datetime:
    return datetime.now(tz=KST)

def parse_datetime(dt_str: Optional[str]) -> Optional[datetime]:
    if not dt_str:
        return None
    try:
        dt = dtparser.parse(dt_str)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except Exception:
        return None

def to_kst_str(dt: Optional[datetime]) -> str:
    if dt is None:
        return ""
    return dt.astimezone(KST).strftime("%Y-%m-%d %H:%M")

def strip_html(text: Optional[str]) -> str:
    text = html.unescape(text or "")
    return BeautifulSoup(text, "html.parser").get_text(separator=" ", strip=True)

def domain_from_url(url: Optional[str]) -> str:
    if not url:
        return ""
    try:
        ext = tldextract.extract(url)
        parts = [p for p in [ext.domain, ext.suffix] if p]
        return ".".join(parts) if parts else ""
    except Exception:
        return ""

def norm_title(t: Optional[str]) -> str:
    t = strip_html(t or "").lower()
    t = re.sub(r"[\[\]„Äê„Äë()ÔºàÔºâ„Äà„Äâ<>„Äé„Äè„Äå„Äç]", " ", t)
    t = re.sub(r"[^\wÍ∞Ä-Ìû£\s]", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t

def content_hash(title: str, content: str) -> str:
    combined = f"{title}:{content[:1000]}"
    return hashlib.md5(combined.encode()).hexdigest()

# =========================
# Ï°∞Ìöå Íµ¨Í∞Ñ
# =========================
def compute_window_utc(now: Optional[datetime] = None) -> Tuple[datetime, datetime]:
    """
    09:00 KST Í∏∞Ï§Ä Ïã§Ìñâ
    - Ìôî~Í∏à: Ï†ÑÎÇ† 09:00 ~ Ïò§Îäò 09:00
    - Ïõî: Í∏àÏöîÏùº 09:00 ~ ÏõîÏöîÏùº 09:00
    """
    now = now or datetime.now(tz=KST)
    anchor_kst = now.astimezone(KST).replace(hour=9, minute=0, second=0, microsecond=0)
    days = 3 if anchor_kst.weekday() == 0 else 1
    start_kst = anchor_kst - timedelta(days=days)
    end_kst = anchor_kst
    return start_kst.astimezone(timezone.utc), end_kst.astimezone(timezone.utc)

# =========================
# ÏãúÌä∏ Î°úÎçî
# =========================
def _split_list(val) -> List[str]:
    if pd.isna(val) or str(val).strip() == "":
        return []
    return [x.strip().lower() for x in str(val).split(",") if x.strip()]

def _query_tokens_from(q: str) -> List[str]:
    if not q:
        return []
    parts = re.split(r'\bOR\b', q, flags=re.IGNORECASE)
    tokens = []
    for p in parts:
        t = p.strip().strip('"').strip("'").lower()
        if t:
            tokens.append(t)
    return tokens

def fetch_org_list() -> List[Dict[str, Any]]:
    sheet_url = os.environ.get("SHEET_CSV_URL", "").strip()
    if not sheet_url:
        raise RuntimeError("SHEET_CSV_URL env var is not set.")

    resp = requests.get(sheet_url, timeout=30)
    resp.raise_for_status()
    csv_text = resp.content.decode("utf-8", errors="replace")
    df = pd.read_csv(StringIO(csv_text))

    name_col = None
    for candidate in ["Ï°∞ÏßÅÎ™Ö", "ÌëúÏãúÎ™Ö"]:
        if candidate in df.columns:
            name_col = candidate
            break
    if not name_col:
        raise RuntimeError("CSVÏóêÎäî Î∞òÎìúÏãú 'Ï°∞ÏßÅÎ™Ö' ÎòêÎäî 'ÌëúÏãúÎ™Ö' Ïó¥Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§.")

    rows: List[Dict[str, Any]] = []
    for _, r in df.iterrows():
        display = str(r[name_col]).strip()
        if not display or display.lower() == "nan":
            continue

        query = str(r.get("Í≤ÄÏÉâÏñ¥", "")).strip() or display
        kind = str(r.get("Ïú†Ìòï", "ORG")).strip().upper() or "ORG"

        must_all = _split_list(r.get("MUST_ALL", ""))
        must_any = _split_list(r.get("MUST_ANY", ""))
        block    = _split_list(r.get("BLOCK", ""))

        item = {
            "display": display,
            "query": query,
            "kind": kind,
            "must_all": must_all,
            "must_any": must_any,
            "block": block,
            "query_tokens": _query_tokens_from(query),
        }
        rows.append(item)

    seen = set(); uniq = []
    for it in rows:
        key = (it["display"], it["query"])
        if key not in seen:
            uniq.append(it); seen.add(key)
    return uniq

# =========================
# Î≥∏Î¨∏ Ï∂îÏ∂ú
# =========================
def fetch_article_text(url: str, timeout: int = 20) -> str:
    if not url:
        return ""
    try:
        downloaded = trafilatura.fetch_url(url, no_ssl=True, timeout=timeout)
        if downloaded:
            text = trafilatura.extract(
                downloaded,
                include_comments=False,
                include_tables=False,
                include_formatting=False,
                favor_recall=True,
                deduplicate=True,
            ) or ""
            return text.strip()
    except Exception:
        pass

    try:
        r = requests.get(url, timeout=timeout, headers={"User-Agent": "Mozilla/5.0"})
        r.raise_for_status()
        return strip_html(r.text)[:8000].strip()
    except Exception:
        return ""

# =========================
# Í≤ÄÏÉâÍ∏∞
# =========================
def search_naver(query: str, display: int = 20) -> List[Dict[str, Any]]:
    cid = os.environ.get("NAVER_CLIENT_ID", "")
    csec = os.environ.get("NAVER_CLIENT_SECRET", "")
    if not cid or not csec:
        return []
    endpoint = "https://openapi.naver.com/v1/search/news.json"
    headers = {"X-Naver-Client-Id": cid, "X-Naver-Client-Secret": csec}
    params = {"query": f"{query}", "display": display, "start": 1, "sort": "date"}
    try:
        r = requests.get(endpoint, headers=headers, params=params, timeout=20)
        r.raise_for_status()
        items = r.json().get("items", [])
        results = []
        for it in items:
            title = strip_html(it.get("title"))
            url = it.get("originallink") or it.get("link")
            pub = parse_datetime(it.get("pubDate"))
            if not url or not title:
                continue
            src = domain_from_url(url) or "naver"
            results.append({
                "title": title,
                "url": url,
                "source": src,
                "published_at": pub,
                "origin": "naver",
                "summary": strip_html(it.get("description", "")),
            })
        return results
    except Exception:
        return []

def search_newsapi(query: str, window_from_utc: datetime, window_to_utc: datetime,
                   language: str = "ko") -> List[Dict[str, Any]]:
    key = os.environ.get("NEWSAPI_KEY", "")
    if not key:
        return []
    endpoint = "https://newsapi.org/v2/everything"
    params = {
        "q": f"{query}",
        "from": window_from_utc.isoformat().replace("+00:00", "Z"),
        "to": window_to_utc.isoformat().replace("+00:00", "Z"),
        "sortBy": "publishedAt",
        "pageSize": 50,
        "language": language,
        "apiKey": key,
    }
    try:
        r = requests.get(endpoint, params=params, timeout=20)
        r.raise_for_status()
        arts = r.json().get("articles", [])
        results = []
        for a in arts:
            title = strip_html(a.get("title"))
            url = a.get("url")
            pub = parse_datetime(a.get("publishedAt"))
            src = (a.get("source") or {}).get("name") or domain_from_url(url)
            if not url or not title:
                continue
            results.append({
                "title": title,
                "url": url,
                "source": src,
                "published_at": pub,
                "origin": "newsapi",
                "summary": strip_html(a.get("description") or a.get("content") or ""),
            })
        return results
    except Exception:
        return []

# =========================
# Í¥ÄÎ†®ÏÑ± ÌïÑÌÑ∞(Ìñâ Í∑úÏπô)
# =========================
def _contains_all(text: str, toks: List[str]) -> bool:
    return all(t in text for t in toks) if toks else True

def _contains_any(text: str, toks: List[str]) -> bool:
    return any(t in text for t in toks) if toks else True

def _contains_none(text: str, toks: List[str]) -> bool:
    return all(t not in text for t in toks) if toks else True

def is_relevant_by_rule(row_cfg: Dict[str, Any], title: str, summary: str) -> bool:
    text = f"{title} {summary}".lower()

    if row_cfg.get("query_tokens") and not _contains_any(text, row_cfg["query_tokens"]):
        return False
    if not _contains_all(text, row_cfg.get("must_all", [])):
        return False
    must_any = row_cfg.get("must_any", [])
    if must_any and not _contains_any(text, must_any):
        return False
    if not _contains_none(text, row_cfg.get("block", [])):
        return False
    return True

# =========================
# Ïª®ÌÖçÏä§Ìä∏ Î∂ÑÏÑù ‚Üí Í∑úÏπô ÎùºÎ≤®
# =========================
def _is_org_related_context(text: str, keywords: List[str], org_name: str) -> bool:
    if not org_name:
        return False
    org = org_name.lower()
    org_positions = [m.start() for m in re.finditer(re.escape(org), text)]
    for kw in keywords:
        for m in re.finditer(re.escape(kw), text):
            for pos in org_positions:
                if abs(pos - m.start()) <= 100:
                    return True
    return False

def analyze_context_signals(title: str, summary: str, content: str, org_name: str) -> Dict[str, Any]:
    full_text = f"{title} {summary} {content}".lower()
    org_mentioned = org_name.lower() in full_text if org_name else False

    signals = {
        "direct_negative": [],
        "contextual_negative": [],
        "monitoring": [],
        "positive": [],
        "safe_activity": [],
        "org_involvement": "direct" if org_mentioned else "indirect",
        "severity_score": 0,
        "confidence": 0.5
    }

    # ÏßÅÏ†ë Î∂ÄÏ†ï(Í∞ÄÏ§ëÏπò 3)
    for category, kws in DIRECT_NEGATIVE.items():
        found = [kw for kw in kws if kw in full_text]
        if found:
            signals["direct_negative"].extend([(category, kw) for kw in found])
            signals["severity_score"] += len(found) * 3

    # ÏÉÅÌô© Î∂ÄÏ†ï(Ï°∞ÏßÅ Í∑ºÏ†ë Ïãú Í∞ÄÏ§ëÏπò 2, ÏïÑÎãàÎ©¥ 1)
    for category, kws in CONTEXTUAL_NEGATIVE.items():
        found = [kw for kw in kws if kw in full_text]
        if found:
            weight = 2 if _is_org_related_context(full_text, found, org_name.lower()) else 1
            signals["contextual_negative"].extend([(category, kw) for kw in found])
            signals["severity_score"] += len(found) * weight

    # Î™®ÎãàÌÑ∞ÎßÅ ÌÇ§ÏõåÎìú(Í∞ÄÏ§ëÏπò 1)
    for category, kws in MONITORING_KEYWORDS.items():
        found = [kw for kw in kws if kw in full_text]
        if found:
            signals["monitoring"].extend([(category, kw) for kw in found])
            signals["severity_score"] += len(found) * 1

    # ÏòàÎ∞©/ÏÉÅÏãú ÌôúÎèô(Í∞ïÌïú Í∏çÏ†ï Í∞ÄÏ§ëÏπò -2)
    sa_found = [kw for kw in SAFE_ACTIVITY_KEYWORDS if kw in full_text]
    if sa_found:
        signals["safe_activity"].extend(sa_found)
        signals["severity_score"] -= len(sa_found) * 2

    # ÏùºÎ∞ò Í∏çÏ†ï(Í∞ÄÏ§ëÏπò -1)
    for category, kws in POSITIVE_KEYWORDS.items():
        found = [kw for kw in kws if kw in full_text]
        if found:
            signals["positive"].extend([(category, kw) for kw in found])
            signals["severity_score"] -= len(found) * 1

    # Ïã†Î¢∞ÎèÑ
    total = (len(signals["direct_negative"]) + len(signals["contextual_negative"])
             + len(signals["monitoring"]) + len(signals["positive"]) + len(signals["safe_activity"]))
    if total > 0:
        if signals["org_involvement"] == "direct":
            signals["confidence"] = min(0.9, 0.5 + total * 0.1)
        else:
            signals["confidence"] = min(0.7, 0.3 + total * 0.05)

    return signals

def enhanced_rule_label(signals: Dict[str, Any]) -> str:
    score = signals["severity_score"]
    conf = signals["confidence"]

    # Í∏çÏ†ï/ÏïàÏ†Ñ Ïã†Ìò∏ Ïö∞ÏÑ∏ ÌåêÎã®
    pos_cnt = len(signals["positive"]) + len(signals["safe_activity"])
    neg_cnt = len(signals["direct_negative"]) + len(signals["contextual_negative"]) + len(signals["monitoring"])
    positive_dominant = pos_cnt > neg_cnt

    # üî¥ Ï°∞Í±¥ ÏôÑÌôî: ÏßÅÏ†ëÎ∂ÄÏ†ï + ÏßÅÏ†ëÏó∞Í¥Ä + Ï†êÏàò‚â•6 + Í∏çÏ†ïÏö∞ÏÑ∏ ÏïÑÎãò
    if (signals["direct_negative"]
        and signals["org_involvement"] == "direct"
        and score >= 6
        and not positive_dominant
        and conf > 0.55):
        return "üî¥"

    # ÏÉÅÌô©Î∂ÄÏ†ïÏúºÎ°úÎßå üî¥Îäî Îçî Î≥¥ÏàòÏ†ÅÏúºÎ°ú
    if (signals["contextual_negative"]
        and signals["org_involvement"] == "direct"
        and score > 7
        and not positive_dominant):
        return "üî¥"

    # Î™ÖÎ∞±Ìïú Í∏çÏ†ï(Ï†êÏàò<0 ÎòêÎäî ÏïàÏ†ÑÏã†Ìò∏ Ï°¥Ïû¨)
    if positive_dominant and score <= 1:
        return "üîµ"

    # Î™®ÎãàÌÑ∞ÎßÅ ÏúÑÏ£º
    if signals["monitoring"] and score > 2 and not positive_dominant:
        return "üü°"

    # Í∏∞Î≥∏ Ï§ëÎ¶Ω
    if score <= 2 or positive_dominant:
        return "üü¢"

    return "üü°"

# =========================
# LLM(JSON) ÎùºÎ≤®
# =========================
IMPACT_MAP = {"positive": "üîµ", "neutral": "üü¢", "monitor": "üü°", "negative": "üî¥"}

def _format_signals_for_llm(signals: Dict[str, Any]) -> str:
    parts = []
    if signals.get("direct_negative"):
        parts.append("ÏßÅÏ†ëÎ∂ÄÏ†ï: " + ", ".join([kw for _, kw in signals["direct_negative"]]))
    if signals.get("contextual_negative"):
        parts.append("ÏÉÅÌô©Î∂ÄÏ†ï: " + ", ".join([kw for _, kw in signals["contextual_negative"]]))
    if signals.get("monitoring"):
        parts.append("Î™®ÎãàÌÑ∞: " + ", ".join([kw for _, kw in signals["monitoring"]]))
    if signals.get("safe_activity"):
        parts.append("ÏòàÎ∞©/ÏÉÅÏãúÌôúÎèô: " + ", ".join(signals["safe_activity"]))
    if signals.get("positive"):
        parts.append("Í∏çÏ†ï: " + ", ".join([kw for _, kw in signals["positive"]]))
    parts.append(f"Ïó∞Í¥ÄÏÑ±: {signals.get('org_involvement','indirect')}")
    parts.append(f"ÏúÑÌóòÏ†êÏàò: {signals.get('severity_score',0)}")
    return " | ".join(parts) if parts else "ÌäπÏù¥ Ïã†Ìò∏ ÏóÜÏùå"

def _safe_load_json(s: str) -> Optional[Dict[str, Any]]:
    try:
        s = re.sub(r'```json\s*|\s*```', '', s)
        return json.loads(s)
    except Exception:
        try:
            m = re.search(r'\{[^}]*"impact"\s*:\s*"([^"]+)"[^}]*\}', s)
            if m:
                return {"impact": m.group(1), "confidence": 0.5}
        except Exception:
            pass
        return None

def llm_enabled() -> bool:
    flag = os.environ.get("LLM_ENABLE", "").strip().lower()
    enabled = flag in {"1", "true", "yes", "on"}
    return enabled and bool(os.environ.get("OPENAI_API_KEY", "").strip()) and _HAS_OPENAI

def enhanced_llm_label(display_name: str, title: str, summary: str, content: str,
                       signals: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    if not llm_enabled():
        return None

    body = (content or "").strip()
    if len(body) > 4000:
        body = body[:4000]

    signal_summary = _format_signals_for_llm(signals)

    try:
        client = openai.OpenAI(api_key=os.environ["OPENAI_API_KEY"].strip())
        model = os.environ.get("LLM_MODEL", "gpt-4o-mini")

        prompt = f"""ÎãπÏã†ÏùÄ Í∏∞ÏóÖ ÏúÑÍ∏∞Í¥ÄÎ¶¨ Ï†ÑÎ¨∏ Î∂ÑÏÑùÍ∞ÄÏûÖÎãàÎã§. Îã§Ïùå Îâ¥Ïä§Í∞Ä 'Ï°∞ÏßÅ'Ïóê ÎØ∏ÏπòÎäî ÏòÅÌñ•ÏùÑ ÌèâÍ∞ÄÌïòÏÑ∏Ïöî.

[ÌèâÍ∞ÄÍ∏∞Ï§Ä]
- positive(üîµ): ÏàòÏÉÅ/Ìà¨Ïûê/ÏÑ±Í≥º/ÌòëÎ†•/ÏòàÎ∞©¬∑ÏÉÅÏãúÏïàÏ†ÑÌôúÎèô Îì± Î™ÖÌôïÌïú Ìò∏Ïû¨
- neutral(üü¢): ÏòÅÌñ• ÎØ∏ÎØ∏, Îã®Ïàú Ïñ∏Í∏â/ÏùºÎ∞ò ÎèôÌñ•
- monitor(üü°): Ï°∞ÏÇ¨/Í≤ÄÌÜ†/Î∂àÌôïÏã§ Îì± Ïû†Ïû¨ Î¶¨Ïä§ÌÅ¨
- negative(üî¥): Î≤ïÏ†Å¬∑ÏÇ¨Í≥† Îì± ÏßÅÏ†ë ÏïÖÏû¨/ÏÇ¨ÏóÖ ÌÉÄÍ≤©

[ÏõêÏπô]
1) Ï°∞ÏßÅÏóê ÎåÄÌïú 'ÏßÅÏ†ë ÏòÅÌñ•' Ïö∞ÏÑ†, ÏóÖÍ≥Ñ ÏùºÎ∞òÎ°†ÏùÄ Í∞ÄÏ§ëÏπò ÎÇÆÏùå
2) Í∏∞ÏÇ¨Ïóê Î™ÖÏãúÎêú ÏÇ¨Ïã§ Í∏∞Î∞ò, Í≥ºÎèÑÌïú Ï∂îÏ∏° Í∏àÏßÄ
3) Ïï†Îß§ÌïòÎ©¥ monitor, Î™ÖÎ∞±Ìûà Í∏çÏ†ïÏùÄ positive
4) ÏòàÎ∞©/ÏÉÅÏãú ÏïàÏ†ÑÌôúÎèôÏùÄ Í∏çÏ†ï ÎòêÎäî Ï§ëÎ¶ΩÏúºÎ°ú Î∂ÑÎ•ò(Î∂ÄÏ†ï ÏïÑÎãò)

[Ï°∞ÏßÅÎ™Ö] {display_name}
[Ï†úÎ™©] {title}
[ÏöîÏïΩ] {summary or "ÏóÜÏùå"}

[Î≥∏Î¨∏(ÏùºÎ∂Ä)]
{body}

[ÏûêÎèô Î∂ÑÏÑù ÏöîÏïΩ]
{signal_summary}

JSONÏúºÎ°úÎßå ÎãµÎ≥Ä:
{{
  "impact": "positive|neutral|monitor|negative",
  "confidence": 0.0,
  "primary_reason": "Ìïú Ï§Ñ Í∑ºÍ±∞",
  "evidence": ["Í∑ºÍ±∞1","Í∑ºÍ±∞2"],
  "org_relevance": "direct|indirect|minimal"
}}"""

        resp = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
            max_tokens=300,
        )
        raw = (resp.choices[0].message.content or "").strip()
        data = _safe_load_json(raw)
        if not data:
            return None

        impact = str(data.get("impact", "")).lower()
        if impact not in IMPACT_MAP:
            return None

        conf = float(data.get("confidence", 0.5))
        return {
            "label": IMPACT_MAP[impact],
            "confidence": conf,
            "raw": data,
            "primary_reason": data.get("primary_reason", ""),
            "org_relevance": data.get("org_relevance", "unknown"),
        }
    except Exception as e:
        logging.error(f"LLM labeling failed: {e}")
        return None

# =========================
# ÌÜµÌï© ÎùºÎ≤®ÎßÅ
# =========================
def integrated_labeling(display_name: str, title: str, summary: str, content: str) -> Dict[str, Any]:
    signals = analyze_context_signals(title, summary, content, display_name)
    rule_lab = enhanced_rule_label(signals)
    llm_res = enhanced_llm_label(display_name, title, summary, content, signals)

    result = {
        "label": rule_lab,
        "confidence": signals["confidence"],
        "method": "rule_based",
        "signals": signals,
        "llm_result": llm_res
    }

    # LLMÏù¥ Ïã†Î¢∞ÎèÑ Ï∂©Î∂ÑÌïòÎ©¥ Ïö∞ÏÑ† Ï†ÅÏö©
    if llm_res and llm_res.get("confidence", 0.0) > 0.6:
        if (signals["direct_negative"]
            and llm_res["label"] in {"üîµ", "üü¢"}
            and signals["org_involvement"] == "direct"):
            result["label"] = "üî¥"
            result["method"] = "conservative_override"
        else:
            result["label"] = llm_res["label"]
            result["confidence"] = llm_res.get("confidence", result["confidence"])
            result["method"] = "llm_primary"

    # Î≥¥Ïàò ÏôÑÌôî: ÏßÅÏ†ëÎ∂ÄÏ†ï ÏóÜÍ≥† Í∏çÏ†ï Ïö∞ÏÑ∏Î©¥ Ìïú Îã®Í≥Ñ ÏôÑÌôî
    pos_cnt = len(signals["positive"]) + len(signals["safe_activity"])
    neg_cnt = len(signals["direct_negative"]) + len(signals["contextual_negative"]) + len(signals["monitoring"])
    positive_dominant = (pos_cnt > neg_cnt)

    if not signals["direct_negative"] and positive_dominant:
        if result["label"] == "üî¥":
            result["label"] = "üü°"
            result["method"] += "_pos_dominant_relief"
        elif result["label"] == "üü°":
            result["label"] = "üü¢"
            result["method"] += "_pos_dominant_relief"

    # Í≥ºÎèÑÌïú üî¥ ÏôÑÌôî(ÏßÅÏ†ëÎ∂ÄÏ†ï ÏóÜÍ≥† Ï†êÏàò ÎÇÆÏùå)
    if (result["label"] == "üî¥"
            and not signals["direct_negative"]
            and signals["severity_score"] < 5):
        result["label"] = "üü°"
        result["method"] += "_moderated"

    return result

# =========================
# Slack
# =========================
def post_to_slack(lines: List[str]) -> None:
    token = os.environ.get("SLACK_BOT_TOKEN", "").strip()
    channel = os.environ.get("SLACK_CHANNEL", "").strip()
    if not token or not channel:
        raise RuntimeError("SLACK_BOT_TOKEN or SLACK_CHANNEL missing.")
    client = WebClient(token=token)
    text = "\n".join(lines) if lines else "Ïò§ÎäòÏùÄ Ïã†Í∑úÎ°ú Í∞êÏßÄÎêú Í∏∞ÏÇ¨Í∞Ä ÏóÜÏäµÎãàÎã§."
    client.chat_postMessage(channel=channel, text=text)

# =========================
# main
# =========================
def main() -> None:
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")

    # Ï£ºÎßê Ïä§ÌÇµ
    if now_kst().weekday() in (5, 6):
        logging.info("Weekend (Sat/Sun) ‚Äì skipping run.")
        return

    window_from_utc, window_to_utc = compute_window_utc()
    logging.info("Window UTC: %s ~ %s", window_from_utc, window_to_utc)

    rows = fetch_org_list()
    logging.info("Loaded %d targets.", len(rows))

    analysis_cache: Dict[str, Dict[str, Any]] = {}
    all_lines: List[str] = []

    for idx, row in enumerate(rows, start=1):
        display = row["display"]
        query = row["query"]
        logging.info("(%d/%d) Searching: %s | %s", idx, len(rows), display, query)

        naver_items = search_naver(query, display=20)
        time.sleep(0.25)
        newsapi_items = search_newsapi(query, window_from_utc, window_to_utc, language="ko")
        logging.info("  raw: naver=%d, newsapi=%d", len(naver_items), len(newsapi_items))

        items: List[Dict[str, Any]] = []
        for it in (naver_items + newsapi_items):
            it["display"] = display
            it["row_cfg"] = row
            items.append(it)

        # Í¥ÄÎ†®ÏÑ± ÌïÑÌÑ∞
        before_rel = len(items)
        items = [it for it in items if is_relevant_by_rule(it["row_cfg"], it["title"], it.get("summary", ""))]
        logging.info("  after relevance: %d -> %d", before_rel, len(items))

        # Í∏∞Í∞Ñ ÌïÑÌÑ∞
        before_win = len(items)
        items = [it for it in items if it["published_at"] and window_from_utc <= it["published_at"] < window_to_utc]
        logging.info("  after window: %d -> %d", before_win, len(items))

        # ÏµúÏã†Ïàú + Ï†úÎ™© dedup
        items.sort(key=lambda x: x["published_at"], reverse=True)
        seen_titles = set()
        uniq: List[Dict[str, Any]] = []
        for it in items:
            title_key = norm_title(it["title"])
            if title_key and it["url"] and title_key not in seen_titles:
                uniq.append(it)
                seen_titles.add(title_key)

        # ÎåÄÌëú Í±¥Ïàò Ï†úÌïú(Ïö∞ÏÑ†ÏàúÏúÑ Ï†úÏô∏)
        if display not in PRIORITY_ORGS:
            uniq = uniq[:3]

        for art in uniq:
            content = fetch_article_text(art["url"])
            cache_key = content_hash(art["title"], content)

            if cache_key in analysis_cache:
                result = dict(analysis_cache[cache_key])  # copy
                logging.info("  Cache hit: %s", art["title"][:50])
            else:
                result = integrated_labeling(
                    art["display"], art["title"], art.get("summary", ""), content
                )
                analysis_cache[cache_key] = result
                logging.info(
                    "  Analyzed: %s -> %s (method=%s, conf=%.2f)",
                    art["title"][:50], result["label"], result["method"], result["confidence"]
                )

            # Ïã†Î¢∞ÎèÑ ÌëúÏãú(?, !)
            conf = float(result.get("confidence", 0.5))
            indicator = "?" if conf < 0.5 else ("!" if conf > 0.8 else "")

            src = art["source"]
            when_str = to_kst_str(art["published_at"])
            line = f"[{art['display']}] <{art['url']}|{art['title']}> ({src})({when_str}) [{result['label']}{indicator}]"
            all_lines.append(line)

    # Î∂ÑÌè¨ Î°úÍ∑∏
    counts = defaultdict(int)
    for ln in all_lines:
        for em in ["üî¥", "üü°", "üü¢", "üîµ"]:
            if em in ln:
                counts[em] += 1
                break
    logging.info("Label distribution: üî¥%d üü°%d üü¢%d üîµ%d",
                 counts["üî¥"], counts["üü°"], counts["üü¢"], counts["üîµ"])

    post_to_slack(all_lines)
    logging.info("Posted %d lines to Slack.", len(all_lines))


if __name__ == "__main__":
    try:
        main()
    except Exception:
        logging.exception("Fatal error")
        raise
