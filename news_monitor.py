#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import re
import html
import time
import logging
import requests
import pandas as pd
from io import StringIO
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
from dateutil import parser as dtparser
from zoneinfo import ZoneInfo
from slack_sdk import WebClient
from slack_sdk.errors import SlackApiError
import tldextract

# --- LLM (OpenAI) ---
try:
    import openai  # pip install openai>=1.40.0
    _HAS_OPENAI = True
except Exception:
    _HAS_OPENAI = False

KST = ZoneInfo("Asia/Seoul")

# ---------- Í≥µÌÜµ Ïú†Ìã∏ ----------
def now_kst():
    return datetime.now(tz=KST)

def parse_datetime(dt_str):
    try:
        dt = dtparser.parse(dt_str)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except Exception:
        return None

def to_kst_str(dt):
    if dt is None:
        return ""
    return dt.astimezone(KST).strftime("%Y-%m-%d %H:%M")

def strip_html(text):
    text = html.unescape(text or "")
    return BeautifulSoup(text, "html.parser").get_text(separator=" ", strip=True)

def domain_from_url(url):
    try:
        ext = tldextract.extract(url)
        parts = [p for p in [ext.domain, ext.suffix] if p]
        return ".".join(parts) if parts else ""
    except Exception:
        return ""

def norm_title(t):
    t = strip_html(t or "").lower()
    t = re.sub(r"[\[\]„Äê„Äë()ÔºàÔºâ„Äà„Äâ<>„Äé„Äè„Äå„Äç]", " ", t)
    t = re.sub(r"[^\wÍ∞Ä-Ìû£\s]", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t

# ---------- Ï°∞Ìöå Íµ¨Í∞Ñ Í≥ÑÏÇ∞ ----------
def compute_window_utc(now=None):
    """
    Îß§Ïùº 09:00 KST Ïã§Ìñâ Í∏∞Ï§Ä Ï°∞Ìöå Íµ¨Í∞Ñ:
    - Ìôî~Í∏à: Ï†ÑÎÇ† 09:00 ~ Ïò§Îäò 09:00
    - Ïõî: Í∏àÏöîÏùº 09:00 ~ ÏõîÏöîÏùº 09:00
    """
    now = now or datetime.now(tz=KST)
    anchor_kst = now.astimezone(KST).replace(hour=9, minute=0, second=0, microsecond=0)
    days = 3 if anchor_kst.weekday() == 0 else 1
    start_kst = anchor_kst - timedelta(days=days)
    end_kst = anchor_kst
    return start_kst.astimezone(timezone.utc), end_kst.astimezone(timezone.utc)

# ---------- ÏãúÌä∏ ÌååÏã± Ïú†Ìã∏ ----------
def _split_list(val):
    if pd.isna(val) or str(val).strip() == "":
        return []
    return [x.strip().lower() for x in str(val).split(",") if x.strip()]

def _query_tokens_from(q):
    # "A" OR "B" ‚Üí ["a","b"] ÏãùÏúºÎ°ú ÌÜ†ÌÅ∞Ìôî (Îî∞Ïò¥ÌëúÎäî Ï†úÍ±∞)
    if not q:
        return []
    parts = re.split(r'\bOR\b', q, flags=re.IGNORECASE)
    tokens = []
    for p in parts:
        t = p.strip().strip('"').strip("'").lower()
        if t:
            tokens.append(t)
    return tokens

# ---------- ÏãúÌä∏ ÏùΩÍ∏∞ ----------
def fetch_org_list():
    """
    Î∞òÌôò: Î¶¨Ïä§Ìä∏[{
      'display': Ïä¨ÎûôÏóê ÌëúÏãúÌï† Ïù¥Î¶Ñ,
      'query': Í≤ÄÏÉâÏñ¥,
      'kind': 'ORG' | 'PERSON',
      'must_all': [...], 'must_any': [...], 'block': [...],
      'query_tokens': [...]
    }, ...]
    * Ï†ïÎ∞Ä Ïª¨ÎüºÏù¥ ÏóÜÏñ¥ÎèÑ ÎèôÏûë(Í∏∞Î≥∏Í∞í Ï†ÅÏö©)
    """
    sheet_url = os.environ.get("SHEET_CSV_URL", "").strip()
    if not sheet_url:
        raise RuntimeError("SHEET_CSV_URL env var is not set.")

    resp = requests.get(sheet_url, timeout=30)
    resp.raise_for_status()
    csv_text = resp.content.decode("utf-8", errors="replace")
    df = pd.read_csv(StringIO(csv_text))

    # ÌïÑÏàò Ïù¥Î¶Ñ Ïª¨Îüº: 'Ï°∞ÏßÅÎ™Ö' ÎòêÎäî 'ÌëúÏãúÎ™Ö' Ï§ë ÌïòÎÇò
    name_col = None
    for candidate in ["Ï°∞ÏßÅÎ™Ö", "ÌëúÏãúÎ™Ö"]:
        if candidate in df.columns:
            name_col = candidate
            break
    if not name_col:
        raise RuntimeError("CSVÏóêÎäî Î∞òÎìúÏãú 'Ï°∞ÏßÅÎ™Ö' ÎòêÎäî 'ÌëúÏãúÎ™Ö' Ïó¥Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§.")

    rows = []
    for _, r in df.iterrows():
        display = str(r[name_col]).strip()
        if not display or display.lower() == "nan":
            continue

        query = str(r.get("Í≤ÄÏÉâÏñ¥", "")).strip() or display
        kind = str(r.get("Ïú†Ìòï", "ORG")).strip().upper() or "ORG"

        must_all = _split_list(r.get("MUST_ALL", ""))
        must_any = _split_list(r.get("MUST_ANY", ""))
        block    = _split_list(r.get("BLOCK", ""))

        item = {
            "display": display,
            "query": query,
            "kind": kind,
            "must_all": must_all,
            "must_any": must_any,
            "block": block,
            "query_tokens": _query_tokens_from(query),
        }
        rows.append(item)

    # Ï§ëÎ≥µ Ï†úÍ±∞(ÌëúÏãúÎ™Ö+ÏøºÎ¶¨ Í∏∞Ï§Ä)
    seen_titles = set()
    uniq = []
    for it in items:
        title_key = norm_title(it["title"])
        if title_key and it["url"] and title_key not in seen_titles:
            uniq.append(it)
            seen_titles.add(title_key)
    return uniq

# ---------- ÎÑ§Ïù¥Î≤Ñ Îâ¥Ïä§ Í≤ÄÏÉâ ----------
def search_naver(query, display=5):
    cid = os.environ.get("NAVER_CLIENT_ID", "")
    csec = os.environ.get("NAVER_CLIENT_SECRET", "")
    if not cid or not csec:
        return []
    endpoint = "https://openapi.naver.com/v1/search/news.json"
    headers = {"X-Naver-Client-Id": cid, "X-Naver-Client-Secret": csec}
    params = {"query": f"{query}", "display": display, "start": 1, "sort": "date"}
    try:
        r = requests.get(endpoint, headers=headers, params=params, timeout=20)
        r.raise_for_status()
        items = r.json().get("items", [])
        results = []
        for it in items:
            title = strip_html(it.get("title"))
            url = it.get("originallink") or it.get("link")
            pub = parse_datetime(it.get("pubDate"))
            if not url or not title:
                continue
            src = domain_from_url(url) or "naver"
            results.append({
                "title": title,
                "url": url,
                "source": src,
                "published_at": pub,
                "origin": "naver",
                "summary": strip_html(it.get("description", "")),
            })
        return results
    except Exception:
        return []

# ---------- NewsAPI ----------
def search_newsapi(query, window_from_utc, window_to_utc, language="ko"):
    key = os.environ.get("NEWSAPI_KEY", "")
    if not key:
        return []
    endpoint = "https://newsapi.org/v2/everything"
    params = {
        "q": f"{query}",
        "from": window_from_utc.isoformat().replace("+00:00","Z"),
        "to": window_to_utc.isoformat().replace("+00:00","Z"),
        "sortBy": "publishedAt",
        "pageSize": 50,
        "language": language,
        "apiKey": key,
    }
    try:
        r = requests.get(endpoint, params=params, timeout=20)
        r.raise_for_status()
        arts = r.json().get("articles", [])
        results = []
        for a in arts:
            title = strip_html(a.get("title"))
            url = a.get("url")
            pub = parse_datetime(a.get("publishedAt"))
            src = (a.get("source") or {}).get("name") or domain_from_url(url)
            if not url or not title:
                continue
            results.append({
                "title": title,
                "url": url,
                "source": src,
                "published_at": pub,
                "origin": "newsapi",
                "summary": strip_html(a.get("description") or a.get("content") or ""),
            })
        return results
    except Exception:
        return []

# ---------- ÎùºÎ≤® Í∑úÏπô (Ìè¥Î∞±Ïö©) ----------
NEG_KW = ["Ìö°Î†π","Î∞∞ÏûÑ","ÏÇ¨Í∏∞","Í≥†Î∞ú","Í∏∞ÏÜå","Íµ¨ÏÜç","ÏàòÏÇ¨","ÏïïÏàòÏàòÏÉâ","ÏÜåÏÜ°","Í≥†ÏÜå","Î∂ÑÏüÅ","Î¶¨ÏΩú","Í≤∞Ìï®","ÏßïÍ≥Ñ","Ï†úÏû¨","Î≤åÍ∏à","Í≥ºÏßïÍ∏à","Î∂ÄÏã§","ÌååÏÇ∞","Î∂ÄÎèÑ","Ï§ëÎã®","Ïó∞Í∏∞","Ïò§Ïóº","ÏÇ¨Îßù","Î∂ÄÏÉÅ","Ìè≠Î∞ú","ÌôîÏû¨","Ï∂îÎùΩ","Ïú†Ï∂ú","Ìï¥ÌÇπ","ÎûúÏÑ¨Ïõ®Ïñ¥","Ïπ®Ìï¥","ÏïÖÏÑ±ÏΩîÎìú","Îã¥Ìï©","ÎèÖÏ†ê","Î∂àÎß§","ÎÖºÎûÄ","Í∞ëÏßà","ÌëúÏ†à","ÌòêÏùò","Î∂àÎ≤ï","ÏúÑÎ≤ï","Ï∑®ÏÜå","Ï≤†Ìöå","Î∂ÄÏ†ï","Ï†ÅÏûê","Í∞êÏÜå","Í∏âÎùΩ","ÌïòÎùΩ","Í≤ΩÍ≥†","Í≤ΩÎ≥¥","Î¶¨Ïä§ÌÅ¨","ÏÜåÌôò","ÏßïÏó≠"]
WATCH_KW = ["ÏùòÌòπ","Ï†êÍ≤Ä","Ï°∞ÏÇ¨","Ïã¨ÏÇ¨","Í≤ÄÌÜ†","ÎÖºÏùò","Ïû†Ï†ï","Ïó∞Íµ¨Í≤∞Í≥º","Ïú†Ïòà","Ïö∞Î†§","Í¥ÄÏã¨","Ï£ºÏãú","Ïû†Ï†ïÏπò","Í≥µÏ†ïÏúÑ","Íµ≠Í∞ê","ÏßÄÏ†Å","ÏöîÍµ¨","Ïó∞Ïû•","Î≥ÄÎèôÏÑ±","Î∂àÌôïÏã§ÏÑ±"]
POS_KW = ["Ìà¨ÏûêÏú†Ïπò","ÏãúÎ¶¨Ï¶à","ÎùºÏö¥Îìú","Ïú†Ïπò","ÏàòÏÉÅ","ÏÑ†Ï†ï","ÌòÅÏã†","Ïã†Í∏∞Î°ù","ÏµúÎåÄ","ÏÉÅÏäπ","Ï¶ùÍ∞Ä","Ìò∏Ï°∞","Ìò∏Ïû¨","ÌôïÎåÄ","ÏßÑÏ∂ú","Ïò§Ìîà","Ï∂úÏãú","Í≥µÍ∞ú","ÌòëÎ†•","ÌååÌä∏ÎÑàÏã≠","MOU","Í≥ÑÏïΩ","ÏàòÏ£º","Îã¨ÏÑ±","ÏÑ±Í≥º","ÌùëÏûê","ÌùëÏûêÏ†ÑÌôò","Í∞úÏµú"]

def rule_label(title, summary):
    text = f"{title} {summary}".lower()
    if any(k.lower() in text for k in NEG_KW): return "üî¥"
    if any(k.lower() in text for k in WATCH_KW): return "üü°"
    if any(k.lower() in text for k in POS_KW): return "üîµ"
    return "üü¢"

# ---------- LLM ÎùºÎ≤®Îü¨ ----------
def llm_enabled():
    return bool(os.environ.get("LLM_ENABLE", "").strip()) and bool(os.environ.get("OPENAI_API_KEY", "").strip()) and _HAS_OPENAI

def llm_label(display_name, title, summary):
    if not llm_enabled(): return None
    try:
        client = openai.OpenAI(api_key=os.environ["OPENAI_API_KEY"].strip())
        model = os.environ.get("LLM_MODEL", "gpt-4o-mini")
        prompt = f"""Îã§Ïùå Í∏∞ÏÇ¨Í∞Ä Ï°∞ÏßÅÏóê ÎØ∏ÏπòÎäî ÏòÅÌñ•ÏùÑ Î∂ÑÎ•òÌïòÏÑ∏Ïöî.
Ï°∞ÏßÅ: {display_name}
Ï†úÎ™©: {title}
ÏöîÏïΩ: {summary}

ÎùºÎ≤® Ï§ë ÌïòÎÇòÎßå Ï∂úÎ†•:
- üîµ (Í∏çÏ†ï)
- üü¢ (Ï§ëÎ¶Ω)
- üü° (ÌåîÎ°úÏóÖ ÌïÑÏöî: Ïû†Ïû¨ Î¶¨Ïä§ÌÅ¨ Í∞ÄÎä•)
- üî¥ (Î∂ÄÏ†ï/Î¶¨Ïä§ÌÅ¨)
Î∞òÎìúÏãú Í∏∞Ìò∏Îßå Ï∂úÎ†•ÌïòÏÑ∏Ïöî."""
        resp = client.chat.completions.create(
            model=model,
            messages=[{"role":"user","content":prompt}],
            temperature=0,
            max_tokens=4,
        )
        out = (resp.choices[0].message.content or "").strip()
        return out if out in {"üîµ","üü¢","üü°","üî¥"} else None
    except Exception:
        return None

# ---------- Ìñâ Í∑úÏπô Í∏∞Î∞ò Í¥ÄÎ†®ÏÑ± ÌïÑÌÑ∞ ----------
def _contains_all(text, toks):    return all(t in text for t in toks) if toks else True
def _contains_any(text, toks):    return any(t in text for t in toks) if toks else True
def _contains_none(text, toks):   return all(t not in text for t in toks) if toks else True

def is_relevant_by_rule(row_cfg, title, summary):
    """
    row_cfg: fetch_org_list()Í∞Ä Î∞òÌôòÌïú Ìïú Ìñâ(dict)
    1) query_tokens Ï§ë ÌïòÎÇòÎäî Î∞òÎìúÏãú Ìè¨Ìï® (Ïù¥Î¶Ñ/Ï°∞ÏßÅ ÏûêÏ≤¥ ÌôïÏù∏)
    2) MUST_ALL Î™®Îëê Ìè¨Ìï®
    3) MUST_ANY Ï§ë ÏµúÏÜå 1Í∞ú Ìè¨Ìï®
    4) BLOCK Îã®Ïñ¥Í∞Ä Ìè¨Ìï®ÎêòÎ©¥ Ï†úÏô∏
    """
    text = f"{title} {summary}".lower()
    if row_cfg["query_tokens"] and not _contains_any(text, row_cfg["query_tokens"]):
        return False
    if not _contains_all(text, row_cfg["must_all"]):
        return False
    if not _contains_any(text, row_cfg["must_any"]):
        return False
    if not _contains_none(text, row_cfg["block"]):
        return False
    return True

# ---------- Slack ----------
def post_to_slack(lines):
    token = os.environ.get("SLACK_BOT_TOKEN", "").strip()
    channel = os.environ.get("SLACK_CHANNEL", "").strip()
    if not token or not channel:
        raise RuntimeError("SLACK_BOT_TOKEN or SLACK_CHANNEL missing.")
    client = WebClient(token=token)
    text = "\n".join(lines) if lines else "Ïò§ÎäòÏùÄ Ïã†Í∑úÎ°ú Í∞êÏßÄÎêú Í∏∞ÏÇ¨Í∞Ä ÏóÜÏäµÎãàÎã§."
    client.chat_postMessage(channel=channel, text=text)

# ---------- main ----------
def main():
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")

    # ‚úÖ ÌÜ†/ÏùºÏù¥Î©¥ Ïä§ÌÇµ (Ïù¥Ï§ë ÏïàÏ†ÑÏû•Ïπò)
    if now_kst().weekday() in (5, 6):  # 5=ÌÜ†, 6=Ïùº
        logging.info("Weekend (Sat/Sun) ‚Äì skipping run.")
        return

    window_from_utc, window_to_utc = compute_window_utc()
    logging.info("Window UTC: %s ~ %s", window_from_utc, window_to_utc)

    max_per_org = int(os.environ.get("MAX_RESULTS_PER_ORG", "1"))

    rows = fetch_org_list()
    logging.info("Loaded %d targets.", len(rows))

    all_lines = []
    for idx, row in enumerate(rows, start=1):
        display = row["display"]
        query   = row["query"]
        logging.info("(%d/%d) Searching: %s | %s", idx, len(rows), display, query)

        naver_items = search_naver(query, display=max(10, max_per_org*4))
        time.sleep(0.25)
        newsapi_items = search_newsapi(query, window_from_utc, window_to_utc, language="ko")
        logging.info("  raw: naver=%d, newsapi=%d", len(naver_items), len(newsapi_items))

        items = []
        for it in (naver_items + newsapi_items):
            it["display"] = display
            it["row_cfg"] = row
            items.append(it)

        # Í¥ÄÎ†®ÏÑ± ÌïÑÌÑ∞
        before_rel = len(items)
        items = [it for it in items if is_relevant_by_rule(it["row_cfg"], it["title"], it.get("summary",""))]
        logging.info("  after relevance: %d -> %d", before_rel, len(items))

        # Í∏∞Í∞Ñ ÌïÑÌÑ∞
        before_win = len(items)
        items = [it for it in items if it["published_at"] and window_from_utc <= it["published_at"] < window_to_utc]
        logging.info("  after window: %d -> %d", before_win, len(items))

        # ÏµúÏã†Ïàú + Ï§ëÎ≥µ Ï†úÍ±∞
        items.sort(key=lambda x: x["published_at"], reverse=True)
        seen = set(); uniq = []
        for it in items:
            key = (norm_title(it["title"]), domain_from_url(it["url"]))
            if key not in seen and it["url"]:
                uniq.append(it); seen.add(key)

        take = uniq

        for art in take:
            label = llm_label(art["display"], art["title"], art.get("summary","")) or \
                    rule_label(art["title"], art.get("summary",""))
            src = art["source"]
            when_str = to_kst_str(art["published_at"])
            line = f"[{art['display']}] <{art['url']}|{art['title']}> ({src})({when_str}) [{label}]"
            all_lines.append(line)

    post_to_slack(all_lines)
    logging.info("Posted %d lines to Slack.", len(all_lines))

if __name__ == "__main__":
    main()
