#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
News Monitor â€“ Gemini+Claude í†µí•© ê°œì„  ë²„ì „
- ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í‚¤ì›Œë“œ ë¶„ì„ (ì§ì ‘ë¶€ì •/ìƒí™©ë¶€ì •/ëª¨ë‹ˆí„°ë§/ê¸ì •)
- LLM í”„ë¡¬í”„íŠ¸ ê°•í™” (primary_reason, org_relevance, confidence)
- ë‹¤ì¸µ ê²€ì¦: ê·œì¹™ â†’ LLM(ì‹ ë¢°ë„) â†’ ë³´ìˆ˜ì  ê²€ì¦ â†’ ê³¼ë„í•œ ğŸ”´ ì™„í™”
- ì£¼ë§ ìŠ¤í‚µ, ì¡°íšŒêµ¬ê°„(ì›”=3ì¼, ê·¸ì™¸=1ì¼), ë³¸ë¬¸ ì¶”ì¶œ(trafilatura)
- ì œëª© ê¸°ì¤€ dedup, ìºì‹±(ì œëª©+ë³¸ë¬¸ í•´ì‹œ)ë¡œ ì¬ë¶„ì„ ë°©ì§€
- Slack ì „ì†¡ ì‹œ ì‹ ë¢°ë„ í‘œì‹œ(!/?), ì£¼ìš” ê·¼ê±° ìš”ì•½
"""

from __future__ import annotations

import os
import re
import html
import time
import json
import logging
import hashlib
import requests
import pandas as pd
from io import StringIO
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
from dateutil import parser as dtparser
from zoneinfo import ZoneInfo
from collections import defaultdict
from slack_sdk import WebClient
from slack_sdk.errors import SlackApiError  # noqa: F401
import tldextract

# ë³¸ë¬¸ ì¶”ì¶œ (ì˜µì…˜)
try:
    import trafilatura
    _HAS_TRAFILATURA = True
except Exception:
    _HAS_TRAFILATURA = False

# OpenAI LLM (ì˜µì…˜)
try:
    import openai  # pip install openai>=1.40.0
    _HAS_OPENAI = True
except Exception:
    _HAS_OPENAI = False

KST = ZoneInfo("Asia/Seoul")

# =========================================================
# ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í‚¤ì›Œë“œ
# =========================================================
DIRECT_NEGATIVE = {
    "ë²•ì ": ["íš¡ë ¹", "ë°°ì„", "ì‚¬ê¸°", "ê³ ë°œ", "ê¸°ì†Œ", "êµ¬ì†", "ìˆ˜ì‚¬", "ì••ìˆ˜ìˆ˜ìƒ‰", "ì§•ì—­", "ì‹¤í˜•", "íŠ¹ê²€"],
    "ì‚¬ì—…": ["ë¦¬ì½œ", "ê²°í•¨", "íŒŒì‚°", "ë¶€ë„", "ì˜ì—…ì •ì§€", "ì‚¬ì—…ì¤‘ë‹¨", "í‡´ì¶œ"],
    "ì•ˆì „": ["ì‚¬ë§", "ë¶€ìƒ", "í­ë°œ", "í™”ì¬", "ì¶”ë½", "ìœ ì¶œ", "í•´í‚¹", "ëœì„¬ì›¨ì–´", "ê°œì¸ì •ë³´ìœ ì¶œ"],
}
CONTEXTUAL_NEGATIVE = {
    "ê²½ì˜": ["ì ì", "ì†ì‹¤", "ê°ì†Œ", "í•˜ë½", "ë¶€ì‹¤"],
    "ê·œì œ": ["ì œì¬", "ë²Œê¸ˆ", "ê³¼ì§•ê¸ˆ", "ì§•ê³„", "ì²˜ë¶„"],
    "ë…¼ë€": ["ë…¼ë€", "ë¹„íŒ", "ê°‘ì§ˆ", "ë¶ˆë²•", "ìœ„ë²•", "ë¶€ì •"],
}
MONITORING_KEYWORDS = {
    "ì¡°ì‚¬": ["ì˜í˜¹", "ì¡°ì‚¬", "ì ê²€", "ì‹¬ì‚¬", "ê²€í† ", "êµ­ê°", "ê°ì‚¬"],
    "ë¶ˆí™•ì‹¤": ["ì—°ê¸°", "ì§€ì—°", "ìœ ì˜ˆ", "ì ì •", "ê²€í† ì¤‘", "ë¶ˆí™•ì‹¤ì„±"],
    "ì£¼ì˜": ["ìš°ë ¤", "ê²½ê³ ", "ë¦¬ìŠ¤í¬", "ë³€ë™ì„±", "ê´€ì‹¬", "ì£¼ì‹œ"],
}
POSITIVE_KEYWORDS = {
    "ì„±ê³¼": ["ìˆ˜ìƒ", "ì„ ì •", "í˜ì‹ ", "ì‹ ê¸°ë¡", "ë‹¬ì„±", "ì„±ê³¼", "í‘ìì „í™˜", "ìµœëŒ€"],
    "ì„±ì¥": ["íˆ¬ììœ ì¹˜", "ìƒìŠ¹", "ì¦ê°€", "í˜¸ì¡°", "í™•ëŒ€", "ì§„ì¶œ", "ì„±ì¥"],
    "í˜‘ë ¥": ["í˜‘ë ¥", "íŒŒíŠ¸ë„ˆì‹­", "mou", "ê³„ì•½", "ìˆ˜ì£¼", "ì œíœ´", "ì—°í•©"],
    "ì‚¬íšŒê³µí—Œ": ["í›„ì›", "ì§€ì›", "ê¸°ë¶€", "ê¸°ì¦", "ê¸°íƒ", "ì¥í•™ê¸ˆ", "ë´‰ì‚¬"],
}

# =========================================================
# ìœ í‹¸
# =========================================================
def now_kst() -> datetime:
    return datetime.now(tz=KST)

def parse_datetime(dt_str: str | None) -> datetime | None:
    if not dt_str:
        return None
    try:
        dt = dtparser.parse(dt_str)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except Exception:
        return None

def to_kst_str(dt: datetime | None) -> str:
    if not dt:
        return ""
    return dt.astimezone(KST).strftime("%Y-%m-%d %H:%M")

def strip_html(text: str | None) -> str:
    text = html.unescape(text or "")
    return BeautifulSoup(text, "html.parser").get_text(separator=" ", strip=True)

def domain_from_url(url: str | None) -> str:
    if not url:
        return ""
    try:
        ext = tldextract.extract(url)
        parts = [p for p in [ext.domain, ext.suffix] if p]
        return ".".join(parts) if parts else ""
    except Exception:
        return ""

def norm_title(t: str | None) -> str:
    t = strip_html(t or "").lower()
    t = re.sub(r"[\[\]ã€ã€‘()ï¼ˆï¼‰ã€ˆã€‰<>ã€ã€ã€Œã€]", " ", t)
    t = re.sub(r"[^\wê°€-í£\s]", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t

def content_hash(title: str, content: str) -> str:
    return hashlib.md5(f"{title}:{content[:1000]}".encode()).hexdigest()

# =========================================================
# ì¡°íšŒ êµ¬ê°„ (09:00 KST ê¸°ì¤€)
# =========================================================
def compute_window_utc(now: datetime | None = None) -> tuple[datetime, datetime]:
    """
    ì‹¤í–‰ ì‹œê°„ 09:00 KST ê¸°ì¤€
    - í™”~ê¸ˆ: ì „ë‚  09:00 ~ ì˜¤ëŠ˜ 09:00
    - ì›”: ê¸ˆ 09:00 ~ ì›” 09:00
    """
    now = now or now_kst()
    anchor_kst = now.astimezone(KST).replace(hour=9, minute=0, second=0, microsecond=0)
    days = 3 if anchor_kst.weekday() == 0 else 1
    start_kst = anchor_kst - timedelta(days=days)
    end_kst = anchor_kst
    return start_kst.astimezone(timezone.utc), end_kst.astimezone(timezone.utc)

# =========================================================
# ì‹œíŠ¸ ë¡œë”© / ê´€ë ¨ì„± í•„í„°
# =========================================================
def _split_list(val) -> list[str]:
    if pd.isna(val) or str(val).strip() == "":
        return []
    return [x.strip().lower() for x in str(val).split(",") if x.strip()]

def _query_tokens_from(q: str) -> list[str]:
    if not q:
        return []
    parts = re.split(r"\bOR\b", q, flags=re.IGNORECASE)
    tokens = []
    for p in parts:
        t = p.strip().strip('"').strip("'").lower()
        if t:
            tokens.append(t)
    return tokens

def fetch_org_list() -> list[dict]:
    """
    CSV í—¤ë”(ì˜ˆ):
    - ì¡°ì§ëª…(ë˜ëŠ” í‘œì‹œëª…), ê²€ìƒ‰ì–´(ì˜µì…˜), ìœ í˜•(ORG|PERSON ì˜µì…˜),
      MUST_ALL, MUST_ANY, BLOCK (ì‰¼í‘œêµ¬ë¶„, ì˜µì…˜)
    """
    sheet_url = os.environ.get("SHEET_CSV_URL", "").strip()
    if not sheet_url:
        raise RuntimeError("SHEET_CSV_URL env var is not set.")

    resp = requests.get(sheet_url, timeout=30)
    resp.raise_for_status()
    csv_text = resp.content.decode("utf-8", errors="replace")
    df = pd.read_csv(StringIO(csv_text))

    name_col = None
    for c in ["ì¡°ì§ëª…", "í‘œì‹œëª…"]:
        if c in df.columns:
            name_col = c
            break
    if not name_col:
        raise RuntimeError("CSVì—ëŠ” ë°˜ë“œì‹œ 'ì¡°ì§ëª…' ë˜ëŠ” 'í‘œì‹œëª…' ì—´ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    rows: list[dict] = []
    for _, r in df.iterrows():
        display = str(r[name_col]).strip()
        if not display or display.lower() == "nan":
            continue
        query = str(r.get("ê²€ìƒ‰ì–´", "")).strip() or display
        kind = str(r.get("ìœ í˜•", "ORG")).strip().upper() or "ORG"
        must_all = _split_list(r.get("MUST_ALL", ""))
        must_any = _split_list(r.get("MUST_ANY", ""))
        block    = _split_list(r.get("BLOCK", ""))

        rows.append({
            "display": display,
            "query": query,
            "kind": kind,
            "must_all": must_all,
            "must_any": must_any,
            "block": block,
            "query_tokens": _query_tokens_from(query),
        })

    # í‘œì‹œëª…+ê²€ìƒ‰ì–´ ê¸°ì¤€ ì¤‘ë³µ ì œê±°
    seen = set(); uniq = []
    for it in rows:
        key = (it["display"], it["query"])
        if key not in seen:
            uniq.append(it); seen.add(key)
    return uniq

def _contains_all(text: str, toks: list[str]) -> bool:
    return all(t in text for t in toks) if toks else True

def _contains_any(text: str, toks: list[str]) -> bool:
    return any(t in text for t in toks) if toks else True

def _contains_none(text: str, toks: list[str]) -> bool:
    return all(t not in text for t in toks) if toks else True

def is_relevant_by_rule(row_cfg: dict, title: str, summary: str) -> bool:
    """
    1) query_tokens ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨(ìˆì„ ë•Œ)
    2) MUST_ALL ëª¨ë‘ í¬í•¨
    3) MUST_ANY ì¤‘ ìµœì†Œ 1ê°œ í¬í•¨(ìˆì„ ë•Œ)
    4) BLOCK í¬í•¨ ì‹œ ì œì™¸
    """
    text = f"{title} {summary}".lower()
    if row_cfg.get("query_tokens") and not _contains_any(text, row_cfg["query_tokens"]):
        return False
    if not _contains_all(text, row_cfg.get("must_all", [])):
        return False
    must_any = row_cfg.get("must_any", [])
    if must_any and not _contains_any(text, must_any):
        return False
    if not _contains_none(text, row_cfg.get("block", [])):
        return False
    return True

# =========================================================
# ë³¸ë¬¸ ì¶”ì¶œ
# =========================================================
def fetch_article_text(url: str, timeout: int = 20) -> str:
    if not url:
        return ""
    # trafilatura ìš°ì„ 
    if _HAS_TRAFILATURA:
        try:
            downloaded = trafilatura.fetch_url(url, no_ssl=True, timeout=timeout)
            if downloaded:
                text = trafilatura.extract(
                    downloaded,
                    include_comments=False,
                    include_tables=False,
                    include_formatting=False,
                    favor_recall=True,
                    deduplicate=True,
                ) or ""
                if text:
                    return text.strip()
        except Exception:
            pass
    # í´ë°±: requests + HTML ì œê±°
    try:
        r = requests.get(url, timeout=timeout, headers={"User-Agent": "Mozilla/5.0"})
        r.raise_for_status()
        return strip_html(r.text)[:8000].strip()
    except Exception:
        return ""

# =========================================================
# ê²€ìƒ‰ê¸°
# =========================================================
def search_naver(query: str, display: int = 20) -> list[dict]:
    cid = os.environ.get("NAVER_CLIENT_ID", "")
    csec = os.environ.get("NAVER_CLIENT_SECRET", "")
    if not cid or not csec:
        return []
    endpoint = "https://openapi.naver.com/v1/search/news.json"
    headers = {"X-Naver-Client-Id": cid, "X-Naver-Client-Secret": csec}
    params = {"query": f"{query}", "display": display, "start": 1, "sort": "date"}
    try:
        r = requests.get(endpoint, headers=headers, params=params, timeout=20)
        r.raise_for_status()
        items = r.json().get("items", [])
        results = []
        for it in items:
            title = strip_html(it.get("title"))
            url = it.get("originallink") or it.get("link")
            pub = parse_datetime(it.get("pubDate"))
            if not url or not title:
                continue
            src = domain_from_url(url) or "naver"
            results.append({
                "title": title,
                "url": url,
                "source": src,
                "published_at": pub,
                "origin": "naver",
                "summary": strip_html(it.get("description", "")),
            })
        return results
    except Exception:
        return []

def search_newsapi(query: str, window_from_utc: datetime, window_to_utc: datetime,
                   language: str = "ko") -> list[dict]:
    key = os.environ.get("NEWSAPI_KEY", "")
    if not key:
        return []
    endpoint = "https://newsapi.org/v2/everything"
    params = {
        "q": f"{query}",
        "from": window_from_utc.isoformat().replace("+00:00", "Z"),
        "to": window_to_utc.isoformat().replace("+00:00", "Z"),
        "sortBy": "publishedAt",
        "pageSize": 50,
        "language": language,
        "apiKey": key,
    }
    try:
        r = requests.get(endpoint, params=params, timeout=20)
        r.raise_for_status()
        arts = r.json().get("articles", [])
        results = []
        for a in arts:
            title = strip_html(a.get("title"))
            url = a.get("url")
            pub = parse_datetime(a.get("publishedAt"))
            src = (a.get("source") or {}).get("name") or domain_from_url(url)
            if not url or not title:
                continue
            results.append({
                "title": title,
                "url": url,
                "source": src,
                "published_at": pub,
                "origin": "newsapi",
                "summary": strip_html(a.get("description") or a.get("content") or ""),
            })
        return results
    except Exception:
        return []

# =========================================================
# ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ & ê·œì¹™ ë¼ë²¨
# =========================================================
def analyze_context_signals(title: str, summary: str, content: str, org_name: str) -> dict:
    text = f"{title} {summary} {content}".lower()
    org_lower = org_name.lower()
    org_involvement = "direct" if org_lower in text else "indirect"

    signals = {
        "direct": [],
        "context": [],
        "mon": [],
        "pos": [],
        "org": org_involvement,
        "score": 0,
        "conf": 0.5,
    }

    # ì§ì ‘ ë¶€ì •: ë†’ì€ ê°€ì¤‘ì¹˜
    for category, kws in DIRECT_NEGATIVE.items():
        found = [kw for kw in kws if kw in text]
        if found:
            signals["direct"].extend((category, kw) for kw in found)
            signals["score"] += 3 * len(found)

    # ìƒí™© ë¶€ì •: ì¡°ì§ ì—°ê´€ì„±ì— ë”°ë¼ ê°€ì¤‘ì¹˜
    for category, kws in CONTEXTUAL_NEGATIVE.items():
        found = [kw for kw in kws if kw in text]
        if found:
            weight = 2 if org_involvement == "direct" else 1
            signals["context"].extend((category, kw) for kw in found)
            signals["score"] += weight * len(found)

    # ëª¨ë‹ˆí„°ë§
    for category, kws in MONITORING_KEYWORDS.items():
        found = [kw for kw in kws if kw in text]
        if found:
            signals["mon"].extend((category, kw) for kw in found)
            signals["score"] += 1 * len(found)

    # ê¸ì •: ì ìˆ˜ ì°¨ê°
    for category, kws in POSITIVE_KEYWORDS.items():
        found = [kw for kw in kws if kw in text]
        if found:
            signals["pos"].extend((category, kw) for kw in found)
            signals["score"] -= 1 * len(found)

    total = len(signals["direct"]) + len(signals["context"]) + len(signals["mon"]) + len(signals["pos"])
    if total:
        signals["conf"] = (min(0.9, 0.5 + 0.1 * total)
                           if org_involvement == "direct"
                           else min(0.7, 0.3 + 0.05 * total))
    return signals

def rule_label_from_signals(sig: dict) -> str:
    if sig["direct"] and sig["conf"] > 0.6:
        return "ğŸ”´"
    if sig["context"] and sig["org"] == "direct" and sig["score"] > 4:
        return "ğŸ”´"
    if sig["mon"] and sig["score"] > 2:
        return "ğŸŸ¡"
    if sig["pos"] and sig["score"] < 0:
        return "ğŸ”µ"
    if sig["score"] <= 2:
        return "ğŸŸ¢"
    return "ğŸŸ¡"

# =========================================================
# LLM ë¼ë²¨ë§
# =========================================================
def llm_enabled() -> bool:
    flag = os.environ.get("LLM_ENABLE", "").strip().lower()
    return (flag in {"1", "true", "yes", "on"}) and bool(os.environ.get("OPENAI_API_KEY", "")) and _HAS_OPENAI

def _format_signals_for_llm(sig: dict) -> str:
    parts = []
    if sig["direct"]:
        parts.append("ì§ì ‘ë¶€ì •: " + ", ".join(kw for _, kw in sig["direct"]))
    if sig["context"]:
        parts.append("ìƒí™©ë¶€ì •: " + ", ".join(kw for _, kw in sig["context"]))
    if sig["mon"]:
        parts.append("ëª¨ë‹ˆí„°ë§: " + ", ".join(kw for _, kw in sig["mon"]))
    if sig["pos"]:
        parts.append("ê¸ì •: " + ", ".join(kw for _, kw in sig["pos"]))
    parts.append(f"ì¡°ì§ì—°ê´€ì„±: {sig['org']} / ìœ„í—˜ì ìˆ˜: {sig['score']}")
    return " | ".join(parts)

def enhanced_llm_label(display: str, title: str, summary: str, content: str, sig: dict) -> dict | None:
    if not llm_enabled():
        return None

    body = (content or "").strip()
    if len(body) > 4000:
        body = body[:4000]
    signal_text = _format_signals_for_llm(sig)

    prompt = f"""
ë‹¹ì‹ ì€ ê¸°ì—… ìœ„ê¸°ê´€ë¦¬ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë‰´ìŠ¤ê°€ '{display}'ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í‰ê°€í•˜ì„¸ìš”.

[íŒë‹¨ê¸°ì¤€]
- positive(ğŸ”µ): ëª…í™•í•œ ê¸ì • ì˜í–¥(ìˆ˜ìƒ, íˆ¬ì, í˜‘ë ¥, í›„ì› ë“±)
- neutral(ğŸŸ¢): ì˜í–¥ ì ìŒ(ì—…ê³„ ì¼ë°˜ ë™í–¥, ë‹¨ìˆœ ì–¸ê¸‰)
- monitor(ğŸŸ¡): ì£¼ì˜ í•„ìš”(ì¡°ì‚¬/ë¶ˆí™•ì‹¤/ì ì¬ ë¦¬ìŠ¤í¬)
- negative(ğŸ”´): ëª…í™•í•œ ë¶€ì • ì˜í–¥(ë²•ì  ë¬¸ì œ, ì‚¬ê³ , ì‚¬ì—… íƒ€ê²©)

[ì›ì¹™]
- ê°•í•œ ë¶€ì • í‚¤ì›Œë“œê°€ ìˆì–´ë„ '{display}'ì™€ ì§ì ‘ ê´€ë ¨ ì—†ìœ¼ë©´ ë¶€ì •ìœ¼ë¡œ íŒë‹¨í•˜ì§€ ë§ ê²ƒ
- ì¶”ì¸¡ ê¸ˆì§€, ê¸°ì‚¬ ë‚´ìš© ê¸°ë°˜ íŒë‹¨
- ì• ë§¤í•˜ë©´ monitor

[ì¡°ì§] {display}
[ì œëª©] {title}
[ìš”ì•½] {summary or "ì—†ìŒ"}
[ë³¸ë¬¸(ì¼ë¶€)] {body}
[ìë™ ë¶„ì„ ì‹ í˜¸] {signal_text}

JSON ì‘ë‹µë§Œ ì¶œë ¥:
{{
  "impact": "positive|neutral|monitor|negative",
  "confidence": 0.0-1.0,
  "primary_reason": "ì£¼ìš” íŒë‹¨ ê·¼ê±°",
  "evidence": ["ê·¼ê±°1","ê·¼ê±°2"],
  "org_relevance": "direct|indirect|minimal"
}}
"""

    try:
        client = openai.OpenAI(api_key=os.environ["OPENAI_API_KEY"].strip())
        model = os.environ.get("LLM_MODEL", "gpt-4o-mini")
        resp = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
            max_tokens=320,
        )
        raw = (resp.choices[0].message.content or "").strip()
        raw = re.sub(r"```json\s*|\s*```", "", raw)  # ì½”ë“œë¸”ë¡ ì œê±°
        data = json.loads(raw)

        im = str(data.get("impact", "")).lower()
        if im not in {"positive", "neutral", "monitor", "negative"}:
            return None
        label_map = {"positive": "ğŸ”µ", "neutral": "ğŸŸ¢", "monitor": "ğŸŸ¡", "negative": "ğŸ”´"}
        return {
            "label": label_map[im],
            "confidence": float(data.get("confidence", 0.5)),
            "primary_reason": data.get("primary_reason", ""),
            "org_relevance": data.get("org_relevance", ""),
            "raw": data,
        }
    except Exception as e:
        logging.error(f"LLM labeling failed: {e}")
        return None

# =========================================================
# í†µí•© ë¼ë²¨ë§
# =========================================================
def integrated_labeling(display: str, title: str, summary: str, content: str) -> dict:
    sig = analyze_context_signals(title, summary, content, display)
    rule = rule_label_from_signals(sig)
    llm = enhanced_llm_label(display, title, summary, content, sig)

    out = {
        "label": rule,
        "confidence": sig["conf"],
        "method": "rule",
        "signals": sig,
        "llm": llm,
    }

    # LLM ì‹ ë¢° ê°€ëŠ¥í•˜ë©´ ìš°ì„  ì ìš©
    if llm and llm["confidence"] > 0.6:
        # ì§ì ‘ ë¶€ì •ì´ ëšœë ·í•œë° LLMì´ ğŸŸ¢/ğŸ”µ â†’ ë³´ìˆ˜ì  ì˜¤ë²„ë¼ì´ë“œ
        if sig["direct"] and llm["label"] in {"ğŸŸ¢", "ğŸ”µ"} and sig["org"] == "direct":
            out["label"] = "ğŸ”´"
            out["method"] = "conservative_override"
        else:
            out["label"] = llm["label"]
            out["confidence"] = llm["confidence"]
            out["method"] = "llm"

    # ê³¼ë„í•œ ğŸ”´ ì™„í™”: ì§ì ‘ ë¶€ì • ì—†ê³  ì ìˆ˜ ë‚®ì„ë•Œ
    if out["label"] == "ğŸ”´" and not sig["direct"] and sig["score"] < 5:
        out["label"] = "ğŸŸ¡"
        out["method"] += "_moderated"

    return out

# =========================================================
# Slack
# =========================================================
def post_to_slack(lines: list[str]) -> None:
    token = os.environ.get("SLACK_BOT_TOKEN", "").strip()
    channel = os.environ.get("SLACK_CHANNEL", "").strip()
    if not token or not channel:
        raise RuntimeError("SLACK_BOT_TOKEN or SLACK_CHANNEL missing.")
    client = WebClient(token=token)
    text = "\n".join(lines) if lines else "ì˜¤ëŠ˜ì€ ì‹ ê·œë¡œ ê°ì§€ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤."
    client.chat_postMessage(channel=channel, text=text)

# =========================================================
# ë©”ì¸
# =========================================================
def main() -> None:
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")

    # ì£¼ë§ ìŠ¤í‚µ(ì´ì¤‘ ì•ˆì „ì¥ì¹˜)
    if now_kst().weekday() in (5, 6):
        logging.info("Weekend (Sat/Sun) â€“ skipping run.")
        return

    window_from_utc, window_to_utc = compute_window_utc()
    logging.info("Window UTC: %s ~ %s", window_from_utc, window_to_utc)

    rows = fetch_org_list()
    logging.info("Loaded %d targets.", len(rows))

    # ìºì‹œ: ê°™ì€ ê¸°ì‚¬(ì œëª©+ë³¸ë¬¸) ì¬ë¶„ì„ ë°©ì§€
    analysis_cache: dict[str, dict] = {}
    all_lines: list[str] = []

    for idx, row in enumerate(rows, start=1):
        display = row["display"]
        query = row["query"]
        logging.info("(%d/%d) Searching: %s | %s", idx, len(rows), display, query)

        naver_items = search_naver(query, display=20)
        time.sleep(0.25)
        newsapi_items = search_newsapi(query, window_from_utc, window_to_utc, language="ko")
        logging.info("  raw: naver=%d, newsapi=%d", len(naver_items), len(newsapi_items))

        items: list[dict] = []
        for it in (naver_items + newsapi_items):
            it["display"] = display
            it["row_cfg"] = row
            items.append(it)

        # 1) ê´€ë ¨ì„± í•„í„°
        before_rel = len(items)
        items = [it for it in items if is_relevant_by_rule(it["row_cfg"], it["title"], it.get("summary", ""))]
        logging.info("  after relevance: %d -> %d", before_rel, len(items))

        # 2) ê¸°ê°„ í•„í„°
        before_win = len(items)
        items = [it for it in items if it["published_at"] and window_from_utc <= it["published_at"] < window_to_utc]
        logging.info("  after window: %d -> %d", before_win, len(items))

        # 3) ìµœì‹ ìˆœ + ì œëª© ê¸°ì¤€ dedup (ë„ë©”ì¸ ë¬´ê´€)
        items.sort(key=lambda x: x["published_at"], reverse=True)
        seen_titles = set(); uniq = []
        for it in items:
            tk = norm_title(it["title"])
            if tk and it["url"] and tk not in seen_titles:
                uniq.append(it); seen_titles.add(tk)

        # 4) ë¼ë²¨ë§
        for art in uniq:
            content = fetch_article_text(art["url"])

            # ìºì‹œ ì²´í¬
            ck = content_hash(art["title"], content)
            if ck in analysis_cache:
                result = analysis_cache[ck].copy()
                logging.info("  Cache hit: %s", art["title"][:60])
            else:
                result = integrated_labeling(art["display"], art["title"], art.get("summary", ""), content)
                analysis_cache[ck] = result
                logging.info("  Analyzed: %s -> %s (method=%s, conf=%.2f)",
                             art["title"][:60], result["label"], result["method"], result["confidence"])

            # Slack ë¼ì¸ ìƒì„±
            label = result["label"]
            conf = result["confidence"]
            conf_mark = "!" if conf > 0.8 else ("?" if conf < 0.5 else "")
            src = art["source"]
            when_str = to_kst_str(art["published_at"])

            extra = []
            llm = result.get("llm") or {}
            if llm.get("primary_reason"):
                extra.append(f"ì´ìœ :{llm['primary_reason']}")
            sig = result.get("signals", {})
            if sig.get("direct"):
                extra.append(f"ì§ì ‘ìœ„í—˜:{len(sig['direct'])}")
            if sig.get("pos"):
                extra.append(f"ê¸ì •ì‹ í˜¸:{len(sig['pos'])}")

            extra_txt = f" ({', '.join(extra)})" if extra else ""
            line = f"[{art['display']}] <{art['url']}|{art['title']}> ({src})({when_str}) [{label}{conf_mark}]{extra_txt}"
            all_lines.append(line)

    # ê²°ê³¼ ë¼ë²¨ ë¶„í¬ ë¡œê·¸
    counts = defaultdict(int)
    for ln in all_lines:
        for emo in ("ğŸ”´", "ğŸŸ¡", "ğŸŸ¢", "ğŸ”µ"):
            if emo in ln:
                counts[emo] += 1
                break
    logging.info("Label distribution: ğŸ”´%d ğŸŸ¡%d ğŸŸ¢%d ğŸ”µ%d",
                 counts["ğŸ”´"], counts["ğŸŸ¡"], counts["ğŸŸ¢"], counts["ğŸ”µ"])

    # Slack ì „ì†¡
    post_to_slack(all_lines)
    logging.info("Posted %d lines to Slack.", len(all_lines))


if __name__ == "__main__":
    try:
        main()
    except Exception:
        logging.exception("Fatal error")
        raise
