#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Daily news monitor: Naver Search API + NewsAPI.org â†’ Slack
- Google Sheets CSV(SHEET_CSV_URL)ì—ì„œ 'ì¡°ì§ëª…' ì—´ì„ ì½ì–´ ì¡°ì§ ë¦¬ìŠ¤íŠ¸ í™•ë³´
- Naver/NewsAPI ì–‘ìª½ì—ì„œ ìµœì‹  ê¸°ì‚¬ ê²€ìƒ‰
- ê´€ë ¨ì„± í•„í„°(ì œëª©/ìš”ì•½ì— ì¡°ì§ëª… ì‹¤ì œ í¬í•¨), ê¸°ê°„ í•„í„°, ì¤‘ë³µ ì œê±°
- ê·œì¹™ ê¸°ë°˜ ë¼ë²¨(ğŸ”´/ğŸŸ¡/ğŸ”µ/ğŸŸ¢) ë¶™ì—¬ Slack ì±„ë„ë¡œ ì „ì†¡

í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜(Secrets/Variables):
  NAVER_CLIENT_ID, NAVER_CLIENT_SECRET, NEWSAPI_KEY
  SLACK_BOT_TOKEN, SLACK_CHANNEL
  SHEET_CSV_URL
ì„ íƒ:
  MAX_RESULTS_PER_ORG (ê¸°ë³¸ 1)
  LOOKBACK_HOURS (ê¸°ë³¸ 24)
"""

import os
import re
import html
import time
import logging
import requests
import pandas as pd
from io import StringIO
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
from dateutil import parser as dtparser
from zoneinfo import ZoneInfo
from slack_sdk import WebClient
from slack_sdk.errors import SlackApiError
import tldextract

KST = ZoneInfo("Asia/Seoul")

# ---------- ê³µí†µ ìœ í‹¸ ----------
def now_kst():
    return datetime.now(tz=KST)

def parse_datetime(dt_str):
    """RFC1123/ISO8601 ë“± ë¬¸ìì—´ì„ tz-aware UTCë¡œ ë³€í™˜."""
    try:
        dt = dtparser.parse(dt_str)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except Exception:
        return None

def to_kst_str(dt):
    if dt is None:
        return ""
    return dt.astimezone(KST).strftime("%Y-%m-%d %H:%M")

def strip_html(text):
    text = html.unescape(text or "")
    return BeautifulSoup(text, "html.parser").get_text(separator=" ", strip=True)

def domain_from_url(url):
    try:
        ext = tldextract.extract(url)
        parts = [p for p in [ext.domain, ext.suffix] if p]
        return ".".join(parts) if parts else ""
    except Exception:
        return ""

def norm_title(t):
    t = strip_html(t or "").lower()
    t = re.sub(r"[\[\]ã€ã€‘()ï¼ˆï¼‰ã€ˆã€‰<>ã€ã€ã€Œã€]", " ", t)
    t = re.sub(r"[^\wê°€-í£\s]", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t

# ---------- ì¡°ì§ ë¦¬ìŠ¤íŠ¸ ----------
def fetch_org_list():
    sheet_url = os.environ.get("SHEET_CSV_URL", "").strip()
    if not sheet_url:
        raise RuntimeError("SHEET_CSV_URL env var is not set.")
    resp = requests.get(sheet_url, timeout=30)
    resp.raise_for_status()

    # ğŸ”§ ë¬¸ìê¹¨ì§ ë°©ì§€: bytes â†’ UTF-8 ê°•ì œ ë””ì½”ë“œ
    csv_text = resp.content.decode("utf-8", errors="replace")
    df = pd.read_csv(StringIO(csv_text))

    # ì•ˆì „ì¥ì¹˜: ë°˜ë“œì‹œ 'ì¡°ì§ëª…' ì—´ì—ì„œ ì½ê¸°
    if "ì¡°ì§ëª…" not in df.columns:
        raise RuntimeError("CSVì— 'ì¡°ì§ëª…' ì—´ì´ í•„ìš”í•©ë‹ˆë‹¤. ì‹œíŠ¸ ì²« ë²ˆì§¸ í—¤ë”ë¥¼ 'ì¡°ì§ëª…'ìœ¼ë¡œ ë°”ê¿”ì£¼ì„¸ìš”.")

    orgs = [
        str(x).strip()
        for x in df["ì¡°ì§ëª…"].tolist()
        if str(x).strip() and str(x).strip().lower() != "nan"
    ]

    # ì¤‘ë³µ ì œê±°(ìˆœì„œ ë³´ì¡´)
    seen = set()
    uniq = []
    for o in orgs:
        if o not in seen:
            uniq.append(o)
            seen.add(o)
    return uniq

# ---------- ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ----------
def search_naver(org, display=5):
    cid = os.environ.get("NAVER_CLIENT_ID", "")
    csec = os.environ.get("NAVER_CLIENT_SECRET", "")
    if not cid or not csec:
        logging.warning("NAVER credentials missing; skipping Naver for %s", org)
        return []
    endpoint = "https://openapi.naver.com/v1/search/news.json"
    headers = {"X-Naver-Client-Id": cid, "X-Naver-Client-Secret": csec}
    params = {"query": f"\"{org}\"", "display": display, "start": 1, "sort": "date"}

    try:
        r = requests.get(endpoint, headers=headers, params=params, timeout=20)
        r.raise_for_status()
        items = r.json().get("items", [])
        results = []
        for it in items:
            title = strip_html(it.get("title"))
            url = it.get("originallink") or it.get("link")
            pub = parse_datetime(it.get("pubDate"))
            if not url or not title:
                continue
            src = domain_from_url(url) or "Naver"
            results.append({
                "org": org,
                "title": title,
                "url": url,
                "source": src,
                "published_at": pub,
                "origin": "naver",
                "summary": strip_html(it.get("description", "")),
            })
        return results
    except Exception as e:
        logging.exception("Naver search failed for %s: %s", org, e)
        return []

# ---------- NewsAPI ----------
def search_newsapi(org, page_size=5, lookback_hours=24, language="ko"):
    key = os.environ.get("NEWSAPI_KEY", "")
    if not key:
        logging.warning("NEWSAPI_KEY missing; skipping NewsAPI for %s", org)
        return []
    endpoint = "https://newsapi.org/v2/everything"
    to_utc = datetime.utcnow().replace(tzinfo=timezone.utc)
    frm = to_utc - timedelta(hours=lookback_hours)
    params = {
        "q": f"\"{org}\"",
        "from": frm.isoformat().replace("+00:00", "Z"),
        "to": to_utc.isoformat().replace("+00:00", "Z"),
        "sortBy": "publishedAt",
        "pageSize": page_size,
        "language": language,  # í•„ìš”ì‹œ 'en' ë“±ìœ¼ë¡œ í™•ì¥
        "apiKey": key,
    }

    try:
        r = requests.get(endpoint, params=params, timeout=20)
        r.raise_for_status()
        arts = r.json().get("articles", [])
        results = []
        for a in arts:
            title = strip_html(a.get("title"))
            url = a.get("url")
            pub = parse_datetime(a.get("publishedAt"))
            src = (a.get("source") or {}).get("name") or domain_from_url(url)
            if not url or not title:
                continue
            results.append({
                "org": org,
                "title": title,
                "url": url,
                "source": src,
                "published_at": pub,
                "origin": "newsapi",
                "summary": strip_html(a.get("description") or a.get("content") or ""),
            })
        return results
    except Exception as e:
        logging.exception("NewsAPI search failed for %s: %s", org, e)
        return []

# ---------- ë¼ë²¨ ê·œì¹™ ----------
NEG_KW = [
    "íš¡ë ¹","ë°°ì„","ì‚¬ê¸°","ê³ ë°œ","ê¸°ì†Œ","êµ¬ì†","ìˆ˜ì‚¬","ì••ìˆ˜ìˆ˜ìƒ‰","ì†Œì†¡","ê³ ì†Œ","ë¶„ìŸ","ë¦¬ì½œ","ê²°í•¨",
    "ì§•ê³„","ì œì¬","ë²Œê¸ˆ","ê³¼ì§•ê¸ˆ","ë¶€ì‹¤","íŒŒì‚°","ë¶€ë„","ì¤‘ë‹¨","ì—°ê¸°","ì˜¤ì—¼","ì‚¬ë§","ë¶€ìƒ","í­ë°œ","í™”ì¬",
    "ì¶”ë½","ìœ ì¶œ","í•´í‚¹","ëœì„¬ì›¨ì–´","ì¹¨í•´","ì•…ì„±ì½”ë“œ","ë‹´í•©","ë…ì ","ë¶ˆë§¤","ë…¼ë€","ê°‘ì§ˆ","í‘œì ˆ","í˜ì˜",
    "ë¶ˆë²•","ìœ„ë²•","ì·¨ì†Œ","ì² íšŒ","ë¶€ì •","ì ì","ì ìì „í™˜","ê°ì†Œ","ê¸‰ë½","í•˜ë½","ê²½ê³ ","ê²½ë³´","ë¦¬ìŠ¤í¬",
    "ë¶€ì •ì ","ê²½ì°°","ê²€ì°°","ë‹¹êµ­","ì†Œí™˜","ì§•ì—­","ê¸ˆì§€","í‡´ì¶œ"
]
WATCH_KW = [
    "ì˜í˜¹","ì ê²€","ì¡°ì‚¬","ì‹¬ì‚¬","ê²€í† ","ë…¼ì˜","ì ì •","ì—°êµ¬ê²°ê³¼","ìœ ì˜ˆ","ìš°ë ¤","ê´€ì‹¬","ì£¼ì‹œ","ì ì •ì¹˜",
    "í•˜í•œ","ìƒí•œ","ê³µì •ìœ„","êµ­ê°","ì§€ì ","ìš”êµ¬","ì—°ì¥","ì—°ë™","ë³€ë™ì„±","ê²½ìŸ ì‹¬í™”","ë¶ˆí™•ì‹¤ì„±"
]
POS_KW = [
    "íˆ¬ììœ ì¹˜","ì‹œë¦¬ì¦ˆ","ë¼ìš´ë“œ","ìœ ì¹˜","ìœ ì¹˜ ì„±ê³µ","ìˆ˜ìƒ","ì„ ì •","ìµœìš°ìˆ˜","í˜ì‹ ","ì‹ ê¸°ë¡","ìµœëŒ€",
    "ìƒìŠ¹","ì¦ê°€","í˜¸ì¡°","í˜¸ì¬","í™•ëŒ€","ì§„ì¶œ","ì˜¤í”ˆ","ì¶œì‹œ","ê³µê°œ","í˜‘ë ¥","íŒŒíŠ¸ë„ˆì‹­","MOU","ê³„ì•½",
    "ìˆ˜ì£¼","ë‹¬ì„±","ê°œìµœ","ì„±ê³¼","ë§¤ì¶œ ì„±ì¥","í‘ì","í‘ìì „í™˜","ìˆ˜ìµì„± ê°œì„ "
]

def label_sentiment(title, summary):
    text = f"{title} {summary}".lower()
    def any_kw(kws):
        for kw in kws:
            if kw.lower() in text:
                return True
        return False
    if any_kw(NEG_KW):
        return "ğŸ”´"
    if any_kw(WATCH_KW):
        return "ğŸŸ¡"
    if any_kw(POS_KW):
        return "ğŸ”µ"
    return "ğŸŸ¢"

# ---------- ì˜¤íƒ(ë¹„ê´€ë ¨) í•„í„° ----------
def is_relevant(org, title, summary):
    """
    ì œëª©/ìš”ì•½ì— ì¡°ì§ëª…ì´ ì‹¤ì œë¡œ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸.
    (SEE:NEAR ê°™ì€ ì˜¤íƒ ì¤„ì´ê¸°)
    """
    text = f"{title} {summary}".lower()
    return org.lower() in text

# ---------- Slack ----------
def post_to_slack(lines):
    token = os.environ.get("SLACK_BOT_TOKEN", "").strip()
    channel = os.environ.get("SLACK_CHANNEL", "").strip()  # '#news-monitor' ë˜ëŠ” 'C...'
    if not token or not channel:
        raise RuntimeError("SLACK_BOT_TOKEN or SLACK_CHANNEL missing.")
    client = WebClient(token=token)
    text = "\n".join(lines)
    try:
        client.chat_postMessage(channel=channel, text=text)
    except SlackApiError as e:
        raise RuntimeError(f"Slack API error: {e.response.get('error')}")

# ---------- main ----------
def main():
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
    lookback = int(os.environ.get("LOOKBACK_HOURS", "24"))
    max_per_org = int(os.environ.get("MAX_RESULTS_PER_ORG", "1"))

    orgs = fetch_org_list()
    logging.info("Loaded %d organizations.", len(orgs))

    all_lines = []
    for idx, org in enumerate(orgs, start=1):
        logging.info("(%d/%d) Searching: %s", idx, len(orgs), org)

        naver_items = search_naver(org, display=max(3, max_per_org*2))
        time.sleep(0.25)  # polite
        newsapi_items = search_newsapi(org, page_size=max(3, max_per_org*2), lookback_hours=lookback, language="ko")

        items = naver_items + newsapi_items

        # 1) ê´€ë ¨ì„± í•„í„°: ì œëª©/ìš”ì•½ì— ì¡°ì§ëª…ì´ ì‹¤ì œ í¬í•¨ëœ ê¸°ì‚¬ë§Œ
        items = [it for it in items if is_relevant(it["org"], it["title"], it.get("summary", ""))]

        # 2) ê¸°ê°„ í•„í„°(ì—„ê²©): ë‚ ì§œ ìˆëŠ” ê¸°ì‚¬ë§Œ lookback ë‚´ ìš°ì„ 
        cutoff = datetime.utcnow().replace(tzinfo=timezone.utc) - timedelta(hours=lookback)
        items_recent = [it for it in items if (it["published_at"] is not None and it["published_at"] >= cutoff)]
        if items_recent:
            items = items_recent
        else:
            # ìµœê·¼ ê¸°ì‚¬ ì „ë¬´ ì‹œì—ë§Œ ë‚ ì§œ ì—†ëŠ” ê²ƒ ì¤‘ ìƒìœ„ ëª‡ ê°œ í—ˆìš©
            items = [it for it in items if it["published_at"] is None][:max_per_org*2]

        # 3) ìµœì‹ ìˆœ ì •ë ¬
        items.sort(key=lambda x: x["published_at"] or datetime.min.replace(tzinfo=timezone.utc), reverse=True)

        # 4) ì¤‘ë³µ ì œê±°(ì •ê·œí™”ëœ ì œëª© + ë„ë©”ì¸)
        seen = set()
        uniq = []
        for it in items:
            key = (norm_title(it["title"]), domain_from_url(it["url"]))
            if key not in seen and it["url"]:
                uniq.append(it); seen.add(key)

        # 5) ì¡°ì§ë‹¹ ìƒìœ„ Nê°œ ì„ íƒ
        take = uniq[:max_per_org]

        for art in take:
            label = label_sentiment(art["title"], art.get("summary",""))
            src = art["source"]
            when_str = to_kst_str(art["published_at"] or now_kst())
            # Slack ë§í¬ í¬ë§·: <url|text>
            line = f"[{art['org']}] <{art['url']}|{art['title']}> ({src})({when_str}) [{label}]"
            all_lines.append(line)

    if not all_lines:
        all_lines.append("ì˜¤ëŠ˜ì€ ì‹ ê·œë¡œ ê°ì§€ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

    post_to_slack(all_lines)
    logging.info("Posted %d lines to Slack.", len(all_lines))

if __name__ == "__main__":
    main()
