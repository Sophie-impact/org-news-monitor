#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
News Monitor â€“ ê°œì„ ëœ ë¼ë²¨ë§ ì‹œìŠ¤í…œ (Claude + Gemini í”¼ë“œë°± ë°˜ì˜)
- ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í‚¤ì›Œë“œ ë¶„ì„ìœ¼ë¡œ ì˜¤íƒ ê°ì†Œ
- LLM í”„ë¡¬í”„íŠ¸ ê°œì„ ìœ¼ë¡œ ì¼ê´€ì„± í–¥ìƒ
- ë‹¤ì¸µ ê²€ì¦ ì‹œìŠ¤í…œìœ¼ë¡œ ê³¼ë„í•œ ìœ„í—˜ ì‹ í˜¸ ë°©ì§€
- ìºì‹±ìœ¼ë¡œ ì¤‘ë³µ ë¶„ì„ ë°©ì§€
- ì¹´ì¹´ì˜¤/ë¸Œë¼ì´ì–¸ì„íŒ©íŠ¸/ê¹€ë²”ìˆ˜ ì œì™¸, ë‚˜ë¨¸ì§€ëŠ” ì¤‘ë³µ ê¸°ì‚¬ í•˜ë‚˜ë§Œ ëŒ€í‘œ í‘œì‹œ
"""

from __future__ import annotations

import os, re, html, time, json, logging, requests, pandas as pd, hashlib
from io import StringIO
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
from dateutil import parser as dtparser
from zoneinfo import ZoneInfo
from slack_sdk import WebClient
from slack_sdk.errors import SlackApiError  # noqa: F401
import tldextract, trafilatura
from collections import defaultdict

# --- LLM (OpenAI) ---
try:
    import openai  # pip install openai>=1.40.0
    _HAS_OPENAI = True
except Exception:
    _HAS_OPENAI = False

KST = ZoneInfo("Asia/Seoul")

# =========================
# ê°œì„ ëœ í‚¤ì›Œë“œ ì‹œìŠ¤í…œ
# =========================

DIRECT_NEGATIVE = {
    "ë²•ì ": ["íš¡ë ¹","ë°°ì„","ì‚¬ê¸°","ê³ ë°œ","ê¸°ì†Œ","êµ¬ì†","ìˆ˜ì‚¬","ì••ìˆ˜ìˆ˜ìƒ‰","íŠ¹ê²€","ì§•ì—­","ì‹¤í˜•"],
    "ì‚¬ì—…": ["ë¦¬ì½œ","ê²°í•¨","íŒŒì‚°","ë¶€ë„","ì˜ì—…ì •ì§€","ì‚¬ì—…ì¤‘ë‹¨","í‡´ì¶œ"],
    "ì•ˆì „": ["ì‚¬ë§","ë¶€ìƒ","í­ë°œ","í™”ì¬","ì¶”ë½","ìœ ì¶œ","í•´í‚¹","ëœì„¬ì›¨ì–´","ê°œì¸ì •ë³´ìœ ì¶œ"],
}

CONTEXTUAL_NEGATIVE = {
    "ê²½ì˜": ["ì ì","ì†ì‹¤","ê°ì†Œ","í•˜ë½","ë¶€ì‹¤"],
    "ê·œì œ": ["ì œì¬","ë²Œê¸ˆ","ê³¼ì§•ê¸ˆ","ì§•ê³„","ì²˜ë¶„"],
    "ë…¼ë€": ["ë…¼ë€","ë¹„íŒ","ê°‘ì§ˆ","ë¶ˆë²•","ìœ„ë²•","ë¶€ì •"],
}

MONITORING_KEYWORDS = {
    "ì¡°ì‚¬": ["ì˜í˜¹","ì¡°ì‚¬","ì ê²€","ì‹¬ì‚¬","ê²€í† ","êµ­ê°","ê°ì‚¬"],
    "ë¶ˆí™•ì‹¤": ["ì—°ê¸°","ì§€ì—°","ìœ ì˜ˆ","ì ì •","ê²€í† ì¤‘","ë¶ˆí™•ì‹¤ì„±"],
    "ì£¼ì˜": ["ìš°ë ¤","ê²½ê³ ","ë¦¬ìŠ¤í¬","ë³€ë™ì„±","ê´€ì‹¬","ì£¼ì‹œ"],
}

POSITIVE_KEYWORDS = {
    "ì„±ê³¼": ["ìˆ˜ìƒ","ì„ ì •","í˜ì‹ ","ì‹ ê¸°ë¡","ìµœëŒ€","ë‹¬ì„±","ì„±ê³¼","í‘ìì „í™˜"],
    "ì„±ì¥": ["íˆ¬ììœ ì¹˜","ì‹œë¦¬ì¦ˆ","ìƒìŠ¹","ì¦ê°€","í˜¸ì¡°","í™•ëŒ€","ì§„ì¶œ","ì„±ì¥"],
    "í˜‘ë ¥": ["í˜‘ë ¥","íŒŒíŠ¸ë„ˆì‹­","mou","ê³„ì•½","ìˆ˜ì£¼","ì œíœ´","ì—°í•©"],
    "ì‚¬íšŒê³µí—Œ": ["í›„ì›","ì§€ì›","ê¸°ë¶€","ê¸°ì¦","ê¸°íƒ","ì¥í•™ê¸ˆ","ë´‰ì‚¬"],
}

# =========================
# ê³µí†µ ìœ í‹¸
# =========================
def now_kst() -> datetime:
    return datetime.now(tz=KST)

def parse_datetime(dt_str: str | None) -> datetime | None:
    if not dt_str:
        return None
    try:
        dt = dtparser.parse(dt_str)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except Exception:
        return None

def to_kst_str(dt: datetime | None) -> str:
    if dt is None:
        return ""
    return dt.astimezone(KST).strftime("%Y-%m-%d %H:%M")

def strip_html(text: str | None) -> str:
    text = html.unescape(text or "")
    return BeautifulSoup(text, "html.parser").get_text(separator=" ", strip=True)

def domain_from_url(url: str | None) -> str:
    if not url:
        return ""
    try:
        ext = tldextract.extract(url)
        parts = [p for p in [ext.domain, ext.suffix] if p]
        return ".".join(parts) if parts else ""
    except Exception:
        return ""

def norm_title(t: str | None) -> str:
    t = strip_html(t or "").lower()
    t = re.sub(r"[\[\]ã€ã€‘()ï¼ˆï¼‰ã€ˆã€‰<>ã€ã€ã€Œã€]", " ", t)
    t = re.sub(r"[^\wê°€-í£\s]", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t

def content_hash(title: str, content: str) -> str:
    combined = f"{title}:{content[:1000]}"
    return hashlib.md5(combined.encode()).hexdigest()

# =========================
# ì¡°íšŒ êµ¬ê°„
# =========================
def compute_window_utc(now: datetime | None = None) -> tuple[datetime, datetime]:
    now = now or datetime.now(tz=KST)
    anchor_kst = now.astimezone(KST).replace(hour=9, minute=0, second=0, microsecond=0)
    days = 3 if anchor_kst.weekday() == 0 else 1
    start_kst = anchor_kst - timedelta(days=days)
    end_kst = anchor_kst
    return start_kst.astimezone(timezone.utc), end_kst.astimezone(timezone.utc)

# =========================
# ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
# =========================
def analyze_context_signals(title: str, summary: str, content: str, org_name: str) -> dict:
    full_text = f"{title} {summary} {content}".lower()
    org_lower = org_name.lower()
    org_mentioned = org_lower in full_text
    signals = {
        "direct_negative": [],
        "contextual_negative": [],
        "monitoring": [],
        "positive": [],
        "org_involvement": "direct" if org_mentioned else "indirect",
        "severity_score": 0,
        "confidence": 0.5,
    }
    for category, keywords in DIRECT_NEGATIVE.items():
        found = [kw for kw in keywords if kw in full_text]
        if found:
            signals["direct_negative"].extend([(category, kw) for kw in found])
            signals["severity_score"] += len(found) * 3
    for category, keywords in CONTEXTUAL_NEGATIVE.items():
        found = [kw for kw in keywords if kw in full_text]
        if found:
            weight = 2 if _is_org_related_context(full_text, found, org_lower) else 1
            signals["contextual_negative"].extend([(category, kw) for kw in found])
            signals["severity_score"] += len(found) * weight
    for category, keywords in MONITORING_KEYWORDS.items():
        found = [kw for kw in keywords if kw in full_text]
        if found:
            signals["monitoring"].extend([(category, kw) for kw in found])
            signals["severity_score"] += len(found)
    for category, keywords in POSITIVE_KEYWORDS.items():
        found = [kw for kw in keywords if kw in full_text]
        if found:
            signals["positive"].extend([(category, kw) for kw in found])
            signals["severity_score"] -= len(found)
    total_signals = len(signals["direct_negative"]) + len(signals["contextual_negative"]) + len(signals["monitoring"]) + len(signals["positive"])
    if total_signals > 0:
        if signals["org_involvement"] == "direct":
            signals["confidence"] = min(0.9, 0.5 + total_signals * 0.1)
        else:
            signals["confidence"] = min(0.7, 0.3 + total_signals * 0.05)
    return signals

def _is_org_related_context(text: str, keywords: list[str], org_name: str) -> bool:
    if not org_name:
        return False
    org_positions = [m.start() for m in re.finditer(re.escape(org_name), text)]
    for kw in keywords:
        kw_positions = [m.start() for m in re.finditer(re.escape(kw), text)]
        for org_pos in org_positions:
            for kw_pos in kw_positions:
                if abs(org_pos - kw_pos) <= 100:
                    return True
    return False

def enhanced_rule_label(signals: dict) -> str:
    score, confidence = signals["severity_score"], signals["confidence"]
    if signals["direct_negative"] and confidence > 0.6:
        return "ğŸ”´"
    if signals["contextual_negative"] and signals["org_involvement"] == "direct" and score > 4:
        return "ğŸ”´"
    if signals["monitoring"] and score > 2:
        return "ğŸŸ¡"
    if signals["positive"] and score < 0:
        return "ğŸ”µ"
    if score <= 2:
        return "ğŸŸ¢"
    return "ğŸŸ¡"

# =========================
# LLM í”„ë¡¬í”„íŠ¸
# =========================
def enhanced_llm_label(display_name: str, title: str, summary: str, content: str, signals: dict) -> dict | None:
    if not llm_enabled():
        return None
    body = (content or "").strip()
    if len(body) > 4000:
        body = body[:4000]
    signal_summary = _format_signals_for_llm(signals)
    try:
        client = openai.OpenAI(api_key=os.environ["OPENAI_API_KEY"].strip())
        model = os.environ.get("LLM_MODEL", "gpt-4o-mini")
        prompt = f"""ë‹¹ì‹ ì€ ê¸°ì—… ìœ„ê¸°ê´€ë¦¬ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ê¸°ì‚¬ê°€ {display_name}ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í‰ê°€í•˜ì„¸ìš”.

í‰ê°€ ê¸°ì¤€:
- positive(ğŸ”µ): ëª…í™•í•œ ê¸ì •ì  ì˜í–¥
- neutral(ğŸŸ¢): ì˜í–¥ë„ ë‚®ìŒ/ì¤‘ë¦½
- monitor(ğŸŸ¡): ì£¼ì˜ê¹Šê²Œ ê´€ì°° í•„ìš”
- negative(ğŸ”´): ëª…í™•í•œ ë¶€ì •ì  ì˜í–¥

ì¡°ì§ëª…: {display_name}
ì œëª©: {title}
ìš”ì•½: {summary or "ì—†ìŒ"}
ë³¸ë¬¸(ì¼ë¶€): {body}
ìë™ ë¶„ì„: {signal_summary}

JSONë§Œ ë°˜í™˜:
{{
  "impact": "positive|neutral|monitor|negative",
  "confidence": 0.0-1.0,
  "primary_reason": "ì£¼ìš” íŒë‹¨ ê·¼ê±°",
  "evidence": ["ê·¼ê±°1","ê·¼ê±°2"],
  "org_relevance": "direct|indirect|minimal"
}}"""
        resp = client.chat.completions.create(
            model=model,
            messages=[{"role":"user","content":prompt}],
            temperature=0.1,
            max_tokens=300,
        )
        raw = (resp.choices[0].message.content or "").strip()
        data = _safe_load_json(raw)
        if not data or "impact" not in data:
            return None
        impact = str(data.get("impact","")).lower()
        if impact not in ["positive","neutral","monitor","negative"]:
            return None
        conf = float(data.get("confidence",0.5))
        label_map = {"positive":"ğŸ”µ","neutral":"ğŸŸ¢","monitor":"ğŸŸ¡","negative":"ğŸ”´"}
        return {
            "label": label_map[impact],
            "confidence": conf,
            "raw": data,
            "primary_reason": data.get("primary_reason",""),
            "org_relevance": data.get("org_relevance","unknown"),
        }
    except Exception as e:
        logging.error(f"LLM labeling failed: {e}")
        return None

def _format_signals_for_llm(signals: dict) -> str:
    parts = []
    if signals["direct_negative"]:
        parts.append(f"ì§ì ‘ì  ë¶€ì •: {', '.join([kw for _,kw in signals['direct_negative']])}")
    if signals["contextual_negative"]:
        parts.append(f"ìƒí™©ì  ë¶€ì •: {', '.join([kw for _,kw in signals['contextual_negative']])}")
    if signals["monitoring"]:
        parts.append(f"ëª¨ë‹ˆí„°ë§: {', '.join([kw for _,kw in signals['monitoring']])}")
    if signals["positive"]:
        parts.append(f"ê¸ì •: {', '.join([kw for _,kw in signals['positive']])}")
    parts.append(f"ì¡°ì§ ì—°ê´€ì„±: {signals['org_involvement']}")
    parts.append(f"ìœ„í—˜ë„: {signals['severity_score']}")
    return " | ".join(parts)

def _safe_load_json(s: str):
    try:
        s = re.sub(r'```json\s*|\s*```','',s)
        return json.loads(s)
    except Exception:
        return None

# =========================
# í†µí•© ë¼ë²¨ë§
# =========================
def integrated_labeling(display_name: str, title: str, summary: str, content: str) -> dict:
    signals = analyze_context_signals(title, summary, content, display_name)
    rule_label = enhanced_rule_label(signals)
    llm_result = enhanced_llm_label(display_name,title,summary,content,signals)
    result = {
        "label": rule_label,
        "confidence": signals["confidence"],
        "method": "rule_based",
        "signals": signals,
        "llm_result": llm_result or {},   # í•­ìƒ dict
    }
    if llm_result and llm_result["confidence"]>0.6:
        if signals["direct_negative"] and llm_result["label"] in {"ğŸŸ¢","ğŸ”µ"} and signals["org_involvement"]=="direct":
            result["label"]="ğŸ”´"; result["method"]="conservative_override"
        else:
            result["label"]=llm_result["label"]; result["confidence"]=llm_result["confidence"]; result["method"]="llm_primary"
    if result["label"]=="ğŸ”´" and not signals["direct_negative"] and signals["severity_score"]<5:
        result["label"]="ğŸŸ¡"; result["method"]+="_moderated"
    return result

# =========================
# ì‹œíŠ¸ ë¡œë”©
# =========================
def _split_list(val)->list[str]:
    if pd.isna(val) or str(val).strip()=="":
        return []
    return [x.strip().lower() for x in str(val).split(",") if x.strip()]

def _query_tokens_from(q:str)->list[str]:
    if not q: return []
    parts=re.split(r'\bOR\b',q,flags=re.IGNORECASE)
    return [p.strip().strip('"').strip("'").lower() for p in parts if p.strip()]

def fetch_org_list()->list[dict]:
    sheet_url=os.environ.get("SHEET_CSV_URL","").strip()
    if not sheet_url: raise RuntimeError("SHEET_CSV_URL not set")
    resp=requests.get(sheet_url,timeout=30); resp.raise_for_status()
    df=pd.read_csv(StringIO(resp.content.decode("utf-8",errors="replace")))
    name_col=None
    for cand in ["ì¡°ì§ëª…","í‘œì‹œëª…"]:
        if cand in df.columns: name_col=cand; break
    if not name_col: raise RuntimeError("CSVì— 'ì¡°ì§ëª…'/'í‘œì‹œëª…' í•„ìš”")
    rows=[]
    for _,r in df.iterrows():
        display=str(r[name_col]).strip()
        if not display or display.lower()=="nan": continue
        query=str(r.get("ê²€ìƒ‰ì–´","")).strip() or display
        kind=str(r.get("ìœ í˜•","ORG")).strip().upper() or "ORG"
        rows.append({
            "display":display,"query":query,"kind":kind,
            "must_all":_split_list(r.get("MUST_ALL","")),
            "must_any":_split_list(r.get("MUST_ANY","")),
            "block":_split_list(r.get("BLOCK","")),
            "query_tokens":_query_tokens_from(query),
        })
    uniq=[]; seen=set()
    for it in rows:
        key=(it["display"],it["query"])
        if key not in seen: uniq.append(it); seen.add(key)
    return uniq

# =========================
# ë³¸ë¬¸ ì¶”ì¶œ / ê²€ìƒ‰
# =========================
def fetch_article_text(url:str,timeout:int=20)->str:
    if not url: return ""
    try:
        dl=trafilatura.fetch_url(url,no_ssl=True,timeout=timeout)
        if dl:
            txt=trafilatura.extract(dl,include_comments=False,include_tables=False,include_formatting=False,favor_recall=True,deduplicate=True) or ""
            return txt.strip()
    except: pass
    try:
        r=requests.get(url,timeout=timeout,headers={"User-Agent":"Mozilla/5.0"}); r.raise_for_status()
        return strip_html(r.text)[:8000].strip()
    except: return ""

def search_naver(query:str,display:int=20)->list[dict]:
    cid,cs=os.environ.get("NAVER_CLIENT_ID",""),os.environ.get("NAVER_CLIENT_SECRET","")
    if not cid or not cs: return []
    endpoint="https://openapi.naver.com/v1/search/news.json"
    headers={"X-Naver-Client-Id":cid,"X-Naver-Client-Secret":cs}
    params={"query":query,"display":display,"start":1,"sort":"date"}
    try:
        r=requests.get(endpoint,headers=headers,params=params,timeout=20); r.raise_for_status()
        items=r.json().get("items",[]); res=[]
        for it in items:
            title=strip_html(it.get("title")); url=it.get("originallink") or it.get("link"); pub=parse_datetime(it.get("pubDate"))
            if not url or not title: continue
            src=domain_from_url(url) or "naver"
            res.append({"title":title,"url":url,"source":src,"published_at":pub,"origin":"naver","summary":strip_html(it.get("description",""))})
        return res
    except: return []

def search_newsapi(query:str,window_from_utc:datetime,window_to_utc:datetime,language:str="ko")->list[dict]:
    key=os.environ.get("NEWSAPI_KEY",""); 
    if not key: return []
    endpoint="https://newsapi.org/v2
