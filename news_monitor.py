#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import re
import html
import time
import logging
import requests
import pandas as pd
from io import StringIO
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
from dateutil import parser as dtparser
from zoneinfo import ZoneInfo
from slack_sdk import WebClient
from slack_sdk.errors import SlackApiError
import tldextract

# --- LLM (OpenAI) ---
try:
    import openai  # pip install openai>=1.40.0
    _HAS_OPENAI = True
except Exception:
    _HAS_OPENAI = False

KST = ZoneInfo("Asia/Seoul")

# ---------- ê³µí†µ ìœ í‹¸ ----------
def now_kst():
    return datetime.now(tz=KST)

def parse_datetime(dt_str):
    try:
        dt = dtparser.parse(dt_str)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except Exception:
        return None

def to_kst_str(dt):
    if dt is None:
        return ""
    return dt.astimezone(KST).strftime("%Y-%m-%d %H:%M")

def strip_html(text):
    text = html.unescape(text or "")
    return BeautifulSoup(text, "html.parser").get_text(separator=" ", strip=True)

def domain_from_url(url):
    try:
        ext = tldextract.extract(url)
        parts = [p for p in [ext.domain, ext.suffix] if p]
        return ".".join(parts) if parts else ""
    except Exception:
        return ""

def norm_title(t):
    t = strip_html(t or "").lower()
    t = re.sub(r"[\[\]ã€ã€‘()ï¼ˆï¼‰ã€ˆã€‰<>ã€ã€ã€Œã€]", " ", t)
    t = re.sub(r"[^\wê°€-í£\s]", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t

# ---------- ì¡°íšŒ êµ¬ê°„ ê³„ì‚° ----------
def compute_window_utc(now=None):
    """
    ë§¤ì¼ 09:00 KST ì‹¤í–‰ ê¸°ì¤€ ì¡°íšŒ êµ¬ê°„:
    - í™”~ê¸ˆ: ì „ë‚  09:00 ~ ì˜¤ëŠ˜ 09:00
    - ì›”: ê¸ˆìš”ì¼ 09:00 ~ ì›”ìš”ì¼ 09:00
    """
    now = now or datetime.now(tz=KST)
    anchor_kst = now.astimezone(KST).replace(hour=9, minute=0, second=0, microsecond=0)

    days = 3 if anchor_kst.weekday() == 0 else 1
    start_kst = anchor_kst - timedelta(days=days)
    end_kst = anchor_kst

    return start_kst.astimezone(timezone.utc), end_kst.astimezone(timezone.utc)

# ---------- ì‹œíŠ¸ ì½ê¸° ----------
def fetch_org_list():
    sheet_url = os.environ.get("SHEET_CSV_URL", "").strip()
    if not sheet_url:
        raise RuntimeError("SHEET_CSV_URL env var is not set.")
    resp = requests.get(sheet_url, timeout=30)
    resp.raise_for_status()

    csv_text = resp.content.decode("utf-8", errors="replace")
    df = pd.read_csv(StringIO(csv_text))

    if "ì¡°ì§ëª…" not in df.columns:
        raise RuntimeError("CSVì— 'ì¡°ì§ëª…' ì—´ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    orgs = []
    for _, row in df.iterrows():
        org = str(row["ì¡°ì§ëª…"]).strip()
        if not org or org.lower() == "nan":
            continue
        query = str(row["ê²€ìƒ‰ì–´"]).strip() if "ê²€ìƒ‰ì–´" in df.columns and str(row["ê²€ìƒ‰ì–´"]).strip() else org
        item = {"org": org, "query": query}
        if item not in orgs:
            orgs.append(item)
    return orgs

# ---------- ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ----------
def search_naver(query, display=5):
    cid = os.environ.get("NAVER_CLIENT_ID", "")
    csec = os.environ.get("NAVER_CLIENT_SECRET", "")
    if not cid or not csec:
        return []
    endpoint = "https://openapi.naver.com/v1/search/news.json"
    headers = {"X-Naver-Client-Id": cid, "X-Naver-Client-Secret": csec}
    params = {"query": f"{query}", "display": display, "start": 1, "sort": "date"}
    try:
        r = requests.get(endpoint, headers=headers, params=params, timeout=20)
        r.raise_for_status()
        items = r.json().get("items", [])
        results = []
        for it in items:
            title = strip_html(it.get("title"))
            url = it.get("originallink") or it.get("link")
            pub = parse_datetime(it.get("pubDate"))
            if not url or not title:
                continue
            src = domain_from_url(url) or "naver"
            results.append({
                "title": title,
                "url": url,
                "source": src,
                "published_at": pub,
                "origin": "naver",
                "summary": strip_html(it.get("description", "")),
            })
        return results
    except:
        return []

# ---------- NewsAPI ----------
def search_newsapi(query, window_from_utc, window_to_utc, language="ko"):
    key = os.environ.get("NEWSAPI_KEY", "")
    if not key:
        return []
    endpoint = "https://newsapi.org/v2/everything"
    params = {
        "q": f"{query}",
        "from": window_from_utc.isoformat().replace("+00:00","Z"),
        "to": window_to_utc.isoformat().replace("+00:00","Z"),
        "sortBy": "publishedAt",
        "pageSize": 50,
        "language": language,
        "apiKey": key,
    }
    try:
        r = requests.get(endpoint, params=params, timeout=20)
        r.raise_for_status()
        arts = r.json().get("articles", [])
        results = []
        for a in arts:
            title = strip_html(a.get("title"))
            url = a.get("url")
            pub = parse_datetime(a.get("publishedAt"))
            src = (a.get("source") or {}).get("name") or domain_from_url(url)
            if not url or not title:
                continue
            results.append({
                "title": title,
                "url": url,
                "source": src,
                "published_at": pub,
                "origin": "newsapi",
                "summary": strip_html(a.get("description") or a.get("content") or ""),
            })
        return results
    except:
        return []

# ---------- ë¼ë²¨ ê·œì¹™ (í´ë°±ìš©) ----------
NEG_KW = ["íš¡ë ¹","ë°°ì„","ì‚¬ê¸°","ê³ ë°œ","ê¸°ì†Œ","êµ¬ì†","ìˆ˜ì‚¬","ì••ìˆ˜ìˆ˜ìƒ‰","ì†Œì†¡","ê³ ì†Œ","ë¶„ìŸ","ë¦¬ì½œ","ê²°í•¨","ì§•ê³„","ì œì¬","ë²Œê¸ˆ","ê³¼ì§•ê¸ˆ","ë¶€ì‹¤","íŒŒì‚°","ë¶€ë„","ì¤‘ë‹¨","ì—°ê¸°","ì˜¤ì—¼","ì‚¬ë§","ë¶€ìƒ","í­ë°œ","í™”ì¬","ì¶”ë½","ìœ ì¶œ","í•´í‚¹","ëœì„¬ì›¨ì–´","ì¹¨í•´","ì•…ì„±ì½”ë“œ","ë‹´í•©","ë…ì ","ë¶ˆë§¤","ë…¼ë€","ê°‘ì§ˆ","í‘œì ˆ","í˜ì˜","ë¶ˆë²•","ìœ„ë²•","ì·¨ì†Œ","ì² íšŒ","ë¶€ì •","ì ì","ê°ì†Œ","ê¸‰ë½","í•˜ë½","ê²½ê³ ","ê²½ë³´","ë¦¬ìŠ¤í¬","ì†Œí™˜","ì§•ì—­"]
WATCH_KW = ["ì˜í˜¹","ì ê²€","ì¡°ì‚¬","ì‹¬ì‚¬","ê²€í† ","ë…¼ì˜","ì ì •","ì—°êµ¬ê²°ê³¼","ìœ ì˜ˆ","ìš°ë ¤","ê´€ì‹¬","ì£¼ì‹œ","ì ì •ì¹˜","ê³µì •ìœ„","êµ­ê°","ì§€ì ","ìš”êµ¬","ì—°ì¥","ë³€ë™ì„±","ë¶ˆí™•ì‹¤ì„±"]
POS_KW = ["íˆ¬ììœ ì¹˜","ì‹œë¦¬ì¦ˆ","ë¼ìš´ë“œ","ìœ ì¹˜","ìˆ˜ìƒ","ì„ ì •","í˜ì‹ ","ì‹ ê¸°ë¡","ìµœëŒ€","ìƒìŠ¹","ì¦ê°€","í˜¸ì¡°","í˜¸ì¬","í™•ëŒ€","ì§„ì¶œ","ì˜¤í”ˆ","ì¶œì‹œ","ê³µê°œ","í˜‘ë ¥","íŒŒíŠ¸ë„ˆì‹­","MOU","ê³„ì•½","ìˆ˜ì£¼","ë‹¬ì„±","ì„±ê³¼","í‘ì","í‘ìì „í™˜","ê°œìµœ"]

def rule_label(title, summary):
    text = f"{title} {summary}".lower()
    if any(k.lower() in text for k in NEG_KW): return "ğŸ”´"
    if any(k.lower() in text for k in WATCH_KW): return "ğŸŸ¡"
    if any(k.lower() in text for k in POS_KW): return "ğŸ”µ"
    return "ğŸŸ¢"

# ---------- LLM ë¼ë²¨ëŸ¬ ----------
def llm_enabled():
    return bool(os.environ.get("LLM_ENABLE", "").strip()) and bool(os.environ.get("OPENAI_API_KEY", "").strip()) and _HAS_OPENAI

def llm_label(org, title, summary):
    if not llm_enabled(): return None
    try:
        client = openai.OpenAI(api_key=os.environ["OPENAI_API_KEY"].strip())
        model = os.environ.get("LLM_MODEL", "gpt-4o-mini")
        prompt = f"""ë‹¤ìŒ ê¸°ì‚¬ê°€ ì¡°ì§ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ë¥˜í•˜ì„¸ìš”.
ì¡°ì§: {org}
ì œëª©: {title}
ìš”ì•½: {summary}

ë¼ë²¨ ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥:
- ğŸ”µ (ê¸ì •)
- ğŸŸ¢ (ì¤‘ë¦½)
- ğŸŸ¡ (íŒ”ë¡œì—… í•„ìš”: ì ì¬ ë¦¬ìŠ¤í¬ ê°€ëŠ¥)
- ğŸ”´ (ë¶€ì •/ë¦¬ìŠ¤í¬)
ë°˜ë“œì‹œ ê¸°í˜¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”."""
        resp = client.chat.completions.create(
            model=model,
            messages=[{"role":"user","content":prompt}],
            temperature=0,
            max_tokens=4,
        )
        out = (resp.choices[0].message.content or "").strip()
        return out if out in {"ğŸ”µ","ğŸŸ¢","ğŸŸ¡","ğŸ”´"} else None
    except:
        return None

# ---------- ì˜¤íƒ í•„í„° ----------
ALLOWED_BRAND_KWS = ["ë¸Œë¼ì´ì–¸ì„íŒ©íŠ¸","ì‚¬ì´ë“œì„íŒ©íŠ¸","brian impact","side impact"]
BLOCK_COMMON = {"í”¼ì•„ë‹ˆìŠ¤íŠ¸","í”¼í”Œ","ì‹œê³µê°„","ëê¹Œì§€ê°„ë‹¤"}

def is_relevant(org, title, summary):
    text = f"{title} {summary}".lower()
    org_l = org.lower()
    if org in BLOCK_COMMON:
        return any(kw.lower() in text for kw in ALLOWED_BRAND_KWS)
    return org_l in text

# ---------- Slack ----------
def post_to_slack(lines):
    token = os.environ.get("SLACK_BOT_TOKEN", "").strip()
    channel = os.environ.get("SLACK_CHANNEL", "").strip()
    if not token or not channel:
        raise RuntimeError("SLACK_BOT_TOKEN or SLACK_CHANNEL missing.")
    client = WebClient(token=token)
    text = "\n".join(lines) if lines else "ì˜¤ëŠ˜ì€ ì‹ ê·œë¡œ ê°ì§€ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤."
    client.chat_postMessage(channel=channel, text=text)

# ---------- main ----------
def main():
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")

    # âœ… í† /ì¼ì´ë©´ ì•„ë¬´ ê²ƒë„ í•˜ì§€ ì•Šê³  ì¢…ë£Œ (ì´ì¤‘ ì•ˆì „ì¥ì¹˜)
    if now_kst().weekday() in (5, 6):  # 5=í† , 6=ì¼
        logging.info("Weekend (Sat/Sun) â€“ skipping run.")
        return

    window_from_utc, window_to_utc = compute_window_utc()
    logging.info("Window UTC: %s ~ %s", window_from_utc, window_to_utc)

    max_per_org = int(os.environ.get("MAX_RESULTS_PER_ORG", "1"))

    org_rows = fetch_org_list()
    logging.info("Loaded %d organizations.", len(org_rows))

    all_lines = []
    for idx, row in enumerate(org_rows, start=1):
        org, query = row["org"], row["query"]
        logging.info("(%d/%d) Searching: %s | %s", idx, len(org_rows), org, query)

        naver_items = search_naver(query, display=max(10, max_per_org*4))
        time.sleep(0.25)
        newsapi_items = search_newsapi(query, window_from_utc, window_to_utc, language="ko")

        items = []
        for it in (naver_items + newsapi_items):
            it["org"] = org
            items.append(it)

        items = [it for it in items if is_relevant(it["org"], it["title"], it.get("summary",""))]
        items = [it for it in items if it["published_at"] and window_from_utc <= it["published_at"] < window_to_utc]

        items.sort(key=lambda x: x["published_at"], reverse=True)

        seen = set(); uniq = []
        for it in items:
            key = (norm_title(it["title"]), domain_from_url(it["url"]))
            if key not in seen and it["url"]:
                uniq.append(it); seen.add(key)

        take = uniq[:max_per_org]

        for art in take:
            label = llm_label(art["org"], art["title"], art.get("summary","")) or rule_label(art["title"], art.get("summary",""))
            src = art["source"]
            when_str = to_kst_str(art["published_at"])
            line = f"[{art['org']}] <{art['url']}|{art['title']}> ({src})({when_str}) [{label}]"
            all_lines.append(line)

    post_to_slack(all_lines)
    logging.info("Posted %d lines to Slack.", len(all_lines))

if __name__ == "__main__":
    main()
